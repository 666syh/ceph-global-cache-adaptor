diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 28ec983..bf4aa6e 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -103,6 +103,11 @@ if(HAVE_INTEL)
     HAVE_BETTER_YASM_ELF64)
 endif()
 
+if(WITH_GLOBAL_CACHE)
+set(third_part_dir /opt/gcache_adaptor_compile/third_part)
+message(STATUS "third part directory --> " ${third_part_dir})
+link_directories(${third_part_dir}/lib)
+endif()
 
 # require c++17
 if(CMAKE_VERSION VERSION_LESS "3.8")
@@ -393,6 +398,10 @@ if(WITH_DPDK)
   list(APPEND ceph_common_deps common_async_dpdk)
 endif()
 
+if(WITH_GLOBAL_CACHE)
+  list(APPEND ceph_common_deps ceph_client_adaptor_plugin)
+endif()
+
 add_library(common STATIC ${ceph_common_objs})
 target_link_libraries(common ${ceph_common_deps})
 
@@ -560,6 +569,12 @@ add_subdirectory(dmclock)
 
 add_subdirectory(compressor)
 
+# Client adaptor
+if(WITH_GLOBAL_CACHE)
+message(STATUS "Client adaptor cmake executing...")
+add_subdirectory(client_adaptor)
+endif()
+
 add_subdirectory(tools)
 
 if(WITH_TESTS)
diff --git a/src/client_adaptor/CMakeLists.txt b/src/client_adaptor/CMakeLists.txt
new file mode 100644
index 0000000..9cf71f1
--- /dev/null
+++ b/src/client_adaptor/CMakeLists.txt
@@ -0,0 +1,17 @@
+set(client_adaptor_srcs
+  ClientAdaptorMsg.cc
+  ClientAdaptorMgr.cc
+  ClientAdaptorPerf.cc
+  ClientAdaptorPlugin.cc
+)
+
+add_library(ceph_client_adaptor_plugin SHARED ${client_adaptor_srcs})
+message(STATUS "In cliend adaptor third part directory --> " ${third_part_dir})
+target_link_libraries(ceph_client_adaptor_plugin osdc ccm_lib das upf ftdsclient dptracepoint dif alarm)
+
+
+set(client_adaptor_dir ${CEPH_INSTALL_PKGLIBDIR})
+install(TARGETS ceph_client_adaptor_plugin DESTINATION ${client_adaptor_dir})
+
+message(STATUS "Global Cache client-adaptor cmake executing...")
+
diff --git a/src/client_adaptor/ClientAdaptorMgr.cc b/src/client_adaptor/ClientAdaptorMgr.cc
new file mode 100644
index 0000000..8de6e67
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMgr.cc
@@ -0,0 +1,197 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include "ClientAdaptorMgr.h"
+
+void ClientAdaptorMgr::set_init_flag(bool flag){
+    init_flag = flag;
+    return;
+}
+
+const bool ClientAdaptorMgr::is_init_succeed(){
+    if(init_flag){
+        return true;
+    } else {
+        return false;
+    }
+}
+
+int32_t CcmPtChangeNotify(PTViewPtEntry *entry, uint32_t entryNum, void *ctx)
+{
+    if (entryNum == 0) {
+        return RET_OK;
+    }
+    std::vector<uint32_t> normal_pt;
+    for (uint32_t i = 0; i < entryNum; i++) {
+        if (entry[i].state == CCM_PT_STATE_OK) {
+            normal_pt.push_back(entry[i].ptId);
+        }
+    }
+    if (normal_pt.size() == 0) {
+        return RET_OK;
+    }
+    Objecter *obj = static_cast<Objecter*>(ctx);
+    obj->retry_op_submit(normal_pt);
+    return RET_OK;
+}
+
+void ClientAdaptorCcm::ccm_deregister(Objecter *obj)
+{
+    if (register_objs.count(obj) > 0) {
+        OpenDeregisterViewChangeNotifyChain(register_objs[obj]);
+        delete register_objs[obj];
+        register_objs.erase(obj);
+    }
+    if (register_objs.empty() && is_init_succeed()) {
+        set_init_flag(false);
+    }
+}
+
+bool ClientAdaptorCcm::ccm_callback_register(Objecter *obj)
+{
+    PTViewChangeOpHandle *ccmCallback = new PTViewChangeOpHandle();
+    register_objs[obj] = ccmCallback;
+    ccmCallback->notifyPtChange = CcmPtChangeNotify;
+    ccmCallback->ctx = (void *)obj;
+    if (OpenRegisterViewChangeNotifyChain(ccmCallback, CCM_MODULE_CLIENT)) {
+        std::cout << __func__ << " Client Adaptor: CCM agent register failed" << std::endl;
+        return false;
+    }
+    return true;
+}
+
+/*
+ * Init manager ,  failed to return !0
+*/
+int32_t ClientAdaptorCcm::init_mgr(Objecter *obj){
+    using namespace std::chrono;
+    int32_t ret = 0;
+    seconds timeout {50 * 60};
+    //Already init
+    if (is_init_succeed()){
+        if (!ccm_callback_register(obj)) {
+            return RET_CCM_REGISTER_ERROR;
+        }
+        return RET_OK;
+    }
+    steady_clock::time_point begin = steady_clock::now();
+    while (true) {
+        ret = OpenAgentInit(false);
+        if (ret == 0) {
+            break;
+        }
+
+        steady_clock::time_point end = steady_clock::now();
+        seconds time_span = duration_cast<seconds>(end - begin);
+        if (time_span > timeout) {
+            std::cout << __func__ << " Client Adaptor: CCM agent init failed because of timeout"
+                                  << ", timeout=" << timeout << std::endl;
+            break;
+        }
+        sleep(1);
+    }
+    if (ret){
+        std::cout << __func__ << " Client Adaptor: CCM agent init failed" << std::endl;
+	    return RET_CCM_AGENT_INIT_ERROR;
+    }
+
+    if (!ccm_callback_register(obj)){
+    	return RET_CCM_REGISTER_ERROR;
+    }
+    return RET_OK;
+}
+
+int32_t ClientAdaptorCcm::get_pt_num(uint32_t& num){
+    num = OpenGetTotalPtNum();
+    return RET_OK;
+}
+
+int32_t ClientAdaptorCcm::get_pt_entry(uint32_t pt_index, PTViewPtEntry* entry){
+    if (OpenGetPtEntry(pt_index, entry)){
+        std::cout << __func__ << " Client Adaptor: Get PT entry failed" << std::endl;
+	    return RET_CCM_PT_ENTRY_ERROR;
+    }
+    return RET_OK;
+}
+
+int32_t ClientAdaptorCcm::get_node_info(uint32_t node_id, NodeInfo* node_info){
+    using namespace std::chrono;
+    int32_t ret = 0;
+    seconds timeout {50 * 60};
+
+    steady_clock::time_point begin = steady_clock::now();
+    while (true) {
+        ret = OpenAgentGetNodeInfo(node_id, node_info);
+        if (ret == 0) {
+            break;
+        }
+
+        steady_clock::time_point end = steady_clock::now();
+        seconds time_span = duration_cast<seconds>(end - begin);
+
+        if (time_span > timeout) {
+            std::cout << __func__ << " Client Adaptor: get node info failed because of timeout"
+                                  << ", timeout=" << timeout << std::endl;
+            break;
+        }
+        sleep(1);
+    }
+    if (ret){
+        std::cout << __func__ << " Client Adaptor: Get node infomation failed" << std::endl;
+        return RET_CCM_NODE_INFO_ERROR;
+    }
+
+    return RET_OK;
+}
+
+bool ClientAdaptorCcm::get_pt_status(uint32_t pt_id) {
+    bool ret = true;
+    PTViewPtEntry pt_entry = { 0 };
+    if (get_pt_entry(pt_id, &pt_entry)) {
+        std::cout << __func__ << " Client Adaptor: Get PT entry failed" << std::endl;
+        return false;
+    }
+
+    if (pt_entry.state != CCM_PT_STATE_OK) {
+        return false;
+    }
+    return ret;
+}
+
+int32_t ClientAdaptorCcm::add_snap_to_gc(const int64_t pool_id,
+                                          const std::string &image_id,
+                                          const uint64_t snap_id) {
+    int32_t ret = OpenCreateSnapshot(pool_id, image_id.c_str(), snap_id);
+    if (ret < 0) {
+        std::cout << __func__ << " Client Adaptor: Add to gc failed, snap_id=" << snap_id
+                 << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorCcm::remove_snap_from_gc(const int64_t pool_id,
+                                          const std::string &image_id,
+                                          const uint64_t snap_id) {
+    int32_t ret = OpenDeleteSnapshot(pool_id, image_id.c_str(), snap_id);
+    if (ret < 0) {
+        std::cout << __func__ << " Client Adaptor: Remove from gc failed, snap_id=" << snap_id
+                 << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorCcm::remove_gc_image_resource(const int64_t pool_id, const std::string image_id) {
+    int32_t ret = 0;//OpenReleaseImageResource(pool_id, image_id.c_str());
+    if (ret < 0) {
+        std::cout << __func__ << " Client Adaptor: Remove image resource failed, image_id=" << image_id
+                 << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
\ No newline at end of file
diff --git a/src/client_adaptor/ClientAdaptorMgr.h b/src/client_adaptor/ClientAdaptorMgr.h
new file mode 100644
index 0000000..387856a
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMgr.h
@@ -0,0 +1,146 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_MGR_H
+#define CLIENT_ADAPTOR_MGR_H
+
+#include <string>
+#include <set>
+#include <osdc/Objecter.h>
+extern "C"
+{
+#include "open_ccm.h"
+}
+
+enum {
+  RET_OK = 0,
+  RET_PARAM_ERROR,
+  RET_CCM_PT_NUM_ERROR,
+  RET_CCM_PT_ENTRY_ERROR,
+  RET_CCM_NODE_INFO_ERROR,
+  RET_CCM_AGENT_INIT_ERROR,
+  RET_CONF_PARSER_ERROR,
+  RET_CCM_PORT_NUM_ERROR,
+  RET_CCM_IP_ERROR,
+  RET_CCM_REGISTER_ERROR,
+  RET_CCM_PARAM_ERROR
+};
+
+class ClientAdaptorMgr {
+public:
+  ClientAdaptorMgr(){}
+  virtual ~ClientAdaptorMgr(){}
+
+  virtual int32_t init_mgr(Objecter *obj) = 0;
+
+  virtual int32_t get_pt_num(uint32_t& num) = 0;
+
+  virtual int32_t get_pt_entry(uint32_t pt_index, PTViewPtEntry* entry) = 0;
+
+  virtual int32_t get_node_info(uint32_t node_id, NodeInfo* node_info) = 0;
+
+  virtual int32_t add_snap_to_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id) = 0;
+
+  virtual int32_t remove_snap_from_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id) = 0;
+
+  virtual int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id) = 0;
+
+  virtual const std::string name(){
+    return "ClientAdaptorMgr";
+  }
+
+  const bool is_init_succeed();
+
+  void set_init_flag(bool flag);
+
+  virtual bool get_pt_status(uint32_t pt_id) = 0;
+  virtual void ccm_deregister(Objecter *obj) = 0;
+private:
+  bool init_flag = false;
+};
+
+class ClientAdaptorCcm : public ClientAdaptorMgr {
+public:
+  ClientAdaptorCcm(){}
+  ~ClientAdaptorCcm() override {}
+
+  int32_t init_mgr(Objecter *obj);  
+
+  int32_t get_pt_num(uint32_t& num);
+
+  int32_t get_pt_entry(uint32_t pt_index, PTViewPtEntry* entry);
+
+  int32_t get_node_info(uint32_t node_id, NodeInfo* node_info);
+
+  int32_t add_snap_to_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id);
+
+  int32_t remove_snap_from_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id);
+
+  int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id);
+
+  const std::string name() override {
+    return "ClientAdaptorCcm";
+  }
+
+  bool get_pt_status(uint32_t pt_id);
+  void ccm_deregister(Objecter *obj);
+
+private:
+  std::map<Objecter*, PTViewChangeOpHandle* > register_objs;
+  bool ccm_callback_register(Objecter *obj);
+}; 
+
+class ClientAdaptorLocal : public ClientAdaptorMgr {
+public:
+  ClientAdaptorLocal(){}
+  ~ClientAdaptorLocal() override {}
+ 
+  int32_t init_mgr(Objecter *obj){
+    return 0;
+  }  
+  
+  int32_t get_pt_num(uint32_t& num){
+    num = 10;
+    return 0;
+  }
+  
+  int32_t get_pt_entry(uint32_t pt_index, PTViewPtEntry* entry){
+    entry->masterNode = pt_index % 3;
+    return 0;
+  }
+
+  int32_t get_node_info(uint32_t node_id, NodeInfo* node_info){
+    strcpy(node_info->publicAddrStr, "localhost");
+    node_info->ports[0] = 1234;
+    node_info->portNum = 1;
+    return 0;
+  }
+
+  int32_t add_snap_to_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id) {
+    return 0;
+  }
+
+  int32_t remove_snap_from_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id)  {
+    return 0;
+  }
+
+  int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id)  {
+    return 0;
+  }
+
+  const std::string name() override {
+    return "ClientAdaptorLocal";
+  }
+  bool get_pt_status(uint32_t pt_id) {
+    return true;
+  }
+  void ccm_deregister(Objecter *obj) {}
+
+private:
+};
+
+
+#endif
diff --git a/src/client_adaptor/ClientAdaptorMsg.cc b/src/client_adaptor/ClientAdaptorMsg.cc
new file mode 100644
index 0000000..001397c
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMsg.cc
@@ -0,0 +1,288 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include <regex>
+#include "ClientAdaptorMsg.h"
+#include "ClientAdaptorMgr.h"
+
+namespace {
+const int RBD_DATA_OBJECT_NAME_FILTER_LEN = 8;  // rbd_data(8 bits)
+const int RBD_DATA_OBJECT_NAME_LEN = 27;   // rbd_data(8 bits).image_id(n bits).object_index(16 bits)
+const string RBD_DATA_OBJECT_NAME = "rbd_data";
+const int RGW_BUCKET_ID_LEN = 36;    // bucket_id(36 bits)
+const int RGW_OBJECT_NAME_LEN = 38;      // bucket_id(36 bits)_Object_Name(n bits)
+const uint64_t SEGMENT_SIZE = 4194304;
+const uint64_t SEGMENT_MASK = 0x3FFFFF;
+const int OBJECT_ID_LEN = 16;
+const int GC_PORT_MIN = 7880;
+const int GC_PORT_MAX = 7889;
+}
+
+ClientAdaptorMsg::ClientAdaptorMsg(ClientAdaptorMgr* mgr) : mgr_ref(mgr){
+}
+
+void ClientAdaptorMsg::push_strategy(Objecter *objecter, uint64_t pool_id, int32_t node_id, std::string oid_name, bufferlist &indata)
+{
+    if (objecter == NULL) {
+        std::cout << __func__ << " Objecter null " << std::endl;
+        return;
+    }
+    if (oid_name.size() == 0) {
+        std::cout << __func__ << " oid_name size 0 " << std::endl;
+        return;
+    }
+    if (node_id < 0) {
+        std::cout << __func__ << " node id < 0 " << std::endl;
+        return;
+    }
+    const char *cls = "rpc";
+    const char *method = "das_prefetch";
+    vector<OSDOp> nops(1);
+    OSDOp &op = nops[0];
+    op.op.op = CEPH_OSD_OP_CALL;
+    op.op.cls.class_len = strlen(cls);
+    op.op.cls.method_len = strlen(method);
+    op.op.cls.indata_len = indata.length();
+    op.indata.append(cls, op.op.cls.class_len);
+    op.indata.append(method, op.op.cls.method_len);
+    op.indata.append(indata);
+    Objecter::Op *objecter_op = 
+	    new Objecter::Op(object_t(oid_name), object_locator_t(), nops, CEPH_OSD_FLAG_EXEC, NULL, NULL, NULL, nullptr);
+    objecter_op->target.osd = node_id;
+    objecter_op->target.base_oloc.pool = pool_id;
+    objecter_op->target.base_oid.name = oid_name;
+    objecter_op->target.flags = CEPH_OSD_FLAG_READ | CEPH_OSD_FLAG_WRITE;
+
+    objecter->op_submit(objecter_op);
+}
+
+/**
+ * Maximum string length of the RBD block object name prefix (not including
+ * null termination).
+ */
+bool ClientAdaptorMsg::filter_msg(Objecter::op_target_t *t){
+
+  string obj_name = t->base_oid.name;	
+  if (obj_name.size() < RBD_DATA_OBJECT_NAME_LEN) {
+    return false;
+  }
+
+  if (obj_name.compare(0, RBD_DATA_OBJECT_NAME_FILTER_LEN, RBD_DATA_OBJECT_NAME) == 0){
+    return true;
+  }
+
+  return false;
+}
+
+bool ClientAdaptorMsg::filter_msg_by_op(Objecter::Op *op){
+  return filter_msg(&op->target);
+}
+
+
+bool ClientAdaptorMsg::is_node(uint32_t index){
+  if (index >> FLAG_OFFSET_BIT){
+    return true;
+  }
+  return false;
+}
+
+int32_t ClientAdaptorMsg::get_node_id(string obj_name, uint64_t& pool_id, uint32_t& pt_id){
+  if (obj_name.length() == 0){
+    std::cout << __func__ << " Client Adaptor: input parameter invalid!" << std::endl;
+    return -RET_CCM_PARAM_ERROR;
+  }
+
+  string obj_id = to_string(pool_id);
+  obj_id += '_';
+  obj_id += obj_name;
+  hash<string> hash_str;
+  uint32_t obj_hashed_id = hash_str(obj_id);
+
+  uint32_t pt_num = 0;
+  NodeInfo info = {0};
+
+  if (mgr_ref->get_pt_num(pt_num)){
+    std::cout << __func__ << " Client Adaptor: Get PT number failed" << std::endl;
+    return -RET_CCM_PT_NUM_ERROR;
+  }
+
+  if (pt_num == 0) {
+    std::cout << __func__ << " Client Adaptor: Get PT number zero" << std::endl;
+    return -RET_CCM_PT_NUM_ERROR;
+  }
+
+  pt_id = obj_hashed_id % pt_num;
+
+  PTViewPtEntry pt_entry = {0};
+  if (mgr_ref->get_pt_entry(pt_id, &pt_entry)){
+    std::cout << __func__ << " Client Adaptor: Get PT entry failed" << std::endl;
+    return -RET_CCM_PT_ENTRY_ERROR;
+  }
+  uint32_t node_id = pt_entry.peerGroup[0].nodeID << NODE_ID_OFFSET_BIT;
+  node_id += 0x1 << FLAG_OFFSET_BIT;
+  pool_id |= pt_entry.birthVersion << 32;
+
+  if (mgr_ref->get_node_info(pt_entry.peerGroup[0].nodeID, &info)){
+    std::cout << __func__ << " Client Adaptor: Get node info failed. node id " << pt_entry.peerGroup[0].nodeID << std::endl;
+    return -RET_CCM_NODE_INFO_ERROR;
+  }
+  if (info.portNum > PORT_SUPPORT_MAX || info.portNum == 0) {
+    std::cout << __func__ << " Client Adaptor: Port number invalid. Port Number: " << info.portNum  << std::endl;
+    return -RET_CCM_PORT_NUM_ERROR;
+  }
+  uint32_t pt_index = pt_entry.indexInNode;
+
+  node_id += pt_index % info.portNum;
+
+  return node_id;
+}
+
+bool ClientAdaptorMsg::valid_ip(string ip_addr)
+{
+    string regStr = "^((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]\\d|[1-9])"\
+                    "(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]\\d|\\d)){3})|(0.0.0.0)$";
+    regex regIp(regStr);
+    bool matchValue = regex_match(ip_addr, regIp);
+    return matchValue;
+}
+
+
+int32_t ClientAdaptorMsg::get_node_ip(uint32_t node_index, string& node_ip){
+  uint32_t node_id = (node_index & NODE_ID_MASK) >> NODE_ID_OFFSET_BIT;
+  uint32_t port_index = node_index & PORT_INDEX_MASK;
+  NodeInfo info = {0};
+  if (mgr_ref->get_node_info(node_id, &info)){
+    std::cout << __func__ << " Client Adaptor: Get node info failed." << std::endl;
+    return RET_CCM_NODE_INFO_ERROR;
+  }
+  uint32_t port = info.ports[port_index];
+  if (port < GC_PORT_MIN || port > GC_PORT_MAX) {
+      return RET_CCM_PORT_NUM_ERROR;
+  }
+
+  string server_ip = info.publicAddrStr;
+
+  if (!valid_ip(server_ip)) {
+      return RET_CCM_IP_ERROR;
+  }
+
+  string addr_str = "tcp://";
+  addr_str += server_ip;
+  addr_str += ":";
+  addr_str += to_string(port);
+  node_ip = addr_str;
+  return RET_OK;
+}
+
+void ClientAdaptorMsg::set_mgr(ClientAdaptorMgr* mgr){
+  mgr_ref = mgr;
+  return;
+}
+
+ClientAdaptorMgr* ClientAdaptorMsg::get_mgr(){
+  return mgr_ref;
+}
+
+void das_req_prefetch(DasKvParam *params)
+{
+    if (params == NULL) {
+        return;
+    }
+    ClientAdaptorMsg *msg_ref = static_cast<ClientAdaptorMsg *>(params->handle);
+    Objecter *obj = static_cast<Objecter *>(params->ctx);
+    if (msg_ref->is_valid_object(obj) ==false) {
+      return;
+    }
+    uint64_t offset = params->offset & SEGMENT_MASK;
+    int id = params->objId;
+    uint64_t left = params->len;
+    while(left) {
+        uint64_t max = std::min<uint64_t>(SEGMENT_SIZE - offset, left);
+
+        char buff[params->imageIdLen+OBJECT_ID_LEN + 1];
+        snprintf(buff, params->imageIdLen + 1, "%s", params->imageIdBuf);
+        snprintf(buff+params->imageIdLen, OBJECT_ID_LEN+1, "%016x", id);
+        std::string oid_name(buff);
+        uint32_t pt_id;
+        uint64_t pool_id = params->cephPoolId;
+        int32_t node_id = msg_ref->get_node_id(oid_name, pool_id, pt_id);
+        if (node_id < 0) {
+            ceph_abort();
+        }
+        bufferlist indata;
+        encode(offset, indata);
+        encode(max, indata);
+        msg_ref->push_strategy(obj, params->cephPoolId, node_id, oid_name, indata);
+
+        left -= max;
+        offset = 0;
+        id++;
+    };
+}
+
+int32_t ClientAdaptorMsg::das_init(Objecter *obj)
+{
+    int32_t rc;
+    das_objs.insert(obj);
+    if (initialized)
+      return 0;
+    DasModuleParam *dasInstanceParam = new DasModuleParam();
+    DasOPS *regOps = new DasOPS();
+    regOps->SubmitDasPrefetch = das_req_prefetch;
+    dasInstanceParam->ops = regOps;
+
+    rc = OpenRcacheCeateDasModule(this, dasInstanceParam);
+    if (rc) {
+      return -1;
+    }
+    initialized = true;
+    return 0;
+}
+
+int32_t ClientAdaptorMsg::das_update_info(Objecter *obj, Objecter::Op *op)
+{
+    if (!initialized)
+      return 0;
+    DasKvParam *params[op->ops.size()];
+
+    if((op->target.flags & CEPH_OSD_FLAG_WRITE) == CEPH_OSD_FLAG_WRITE)
+      return 0;
+    string obj_name = op->target.base_oid.name;
+    if (obj_name.compare(0, RBD_DATA_OBJECT_NAME_FILTER_LEN, RBD_DATA_OBJECT_NAME))
+      return 0;
+    std::size_t found = obj_name.find_last_of('.');
+    if(found == std::string::npos)
+      return -EINVAL;
+    uint64_t objId = std::stol(obj_name.substr(found+1, OBJECT_ID_LEN), nullptr, 16);
+    uint64_t ns = ceph_clock_now().to_nsec();
+    int i = 0;
+    for(vector<OSDOp>::iterator p = op->ops.begin(); p != op->ops.end(); ++p) {
+    	if (p->op.op == CEPH_OSD_OP_READ || p->op.op == CEPH_OSD_OP_SPARSE_READ || p->op.op == CEPH_OSD_OP_SYNC_READ) {
+            params[i] = reinterpret_cast<DasKvParam*>(new char[sizeof(DasKvParam) + found + 1]);
+            params[i]->offset = p->op.extent.offset;
+            params[i]->len = p->op.extent.length;
+            params[i]->opcode = 0;
+            params[i]->timeStamp = ns;
+            params[i]->cephPoolId = op->target.base_oloc.pool;
+            params[i]->algType = DAS_ALG_SEQ;
+            params[i]->objId = objId;
+            params[i]->imageIdLen =found + 1;
+            memcpy(params[i]->imageIdBuf, obj_name.c_str(), params[i]->imageIdLen);
+            params[i]->handle = this;
+            params[i]->ctx = obj;
+            i++;
+        }
+    }
+
+    if (i) {
+      int rc = OpenRcachePutDasInfo(params, i);
+      if (rc)
+	 return -1;
+    }
+    return 0;
+}
+
diff --git a/src/client_adaptor/ClientAdaptorMsg.h b/src/client_adaptor/ClientAdaptorMsg.h
new file mode 100644
index 0000000..585d06d
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMsg.h
@@ -0,0 +1,77 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_MSG_H
+#define CLIENT_ADAPTOR_MSG_H
+
+#include <string>
+#include <map>
+#include <stdint.h>
+#include "open_das.h"
+
+#include "osdc/Objecter.h"
+#include "ClientAdaptorMgr.h"
+
+class ClientAdaptorMsg {
+public:
+  ClientAdaptorMsg(ClientAdaptorMgr* mgr); 
+  ~ClientAdaptorMsg() {}; 
+
+  void push_strategy(Objecter *objecter, uint64_t pool_id, int32_t node_id, std::string oid_name, bufferlist &indata);
+
+  int32_t das_init(Objecter *obj);
+
+  void das_remove(Objecter *obj) {
+    das_objs.erase(obj);
+    if (das_objs.empty() && initialized) {
+      initialized = false;
+      OpenRcacheExitDasModule(this);
+    }
+  }
+
+  bool filter_msg(Objecter::op_target_t *t);
+
+  bool filter_msg_by_op(Objecter::Op *op);
+
+  const string name() {return "ClientAdaptorMsg";}
+
+  bool is_node(uint32_t index);
+
+  int32_t get_node_id(string obj_name, uint64_t& pool_id, uint32_t& pt_index);
+
+  int32_t get_node_ip(uint32_t node_index, string& node_ip);
+
+  void set_mgr(ClientAdaptorMgr* mgr);
+
+  ClientAdaptorMgr* get_mgr(void);
+
+  bool is_valid_object(Objecter *obj) {
+    auto it = das_objs.find(obj);
+    if (it != das_objs.end())
+      return true;
+    return false;
+  }
+
+  int32_t das_update_info(Objecter *obj, Objecter::Op *op);
+
+protected:
+  ClientAdaptorMgr* mgr_ref;
+
+  const int FLAG_OFFSET_BIT = 20;
+  const int PORT_INDEX_MASK = 0xf;
+  const int NODE_ID_OFFSET_BIT = 4;
+  const int NODE_ID_MASK = 0xffff0;
+  const int PORT_SUPPORT_MAX = 16;
+private:
+  bool initialized = false;
+  std::set<Objecter *> das_objs;
+  bool valid_ip(string ip_addr);
+public:
+  std::unordered_set<void *> connections;
+};
+
+
+#endif
diff --git a/src/client_adaptor/ClientAdaptorPerf.cc b/src/client_adaptor/ClientAdaptorPerf.cc
new file mode 100644
index 0000000..cea7835
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPerf.cc
@@ -0,0 +1,85 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include "ClientAdaptorPerf.h"
+#include "ClientAdaptorPlugin.h"
+using namespace std;
+
+#include <sys/syscall.h>
+#define gettid() syscall(__NR_gettid)
+
+void ClientAdaptorPerf::start_tick(Objecter::Op *op) {
+  gettimeofday(&(op->perf_tick.start), NULL);
+  return;
+}
+
+void ClientAdaptorPerf::end_tick(Objecter::Op *op) {
+  gettimeofday(&(op->perf_tick.end), NULL);
+  return;
+}
+  
+void ClientAdaptorPerf::record_op(Objecter::Op *op) {
+  for (vector<OSDOp>::iterator p = op->ops.begin(); p != op->ops.end(); ++p) {
+    if (p->op.op == CEPH_OSD_OP_READ || p->op.op == CEPH_OSD_OP_SPARSE_READ || p->op.op == CEPH_OSD_OP_SYNC_READ) {
+      read.op_count++;
+      read.time_cost += (op->perf_tick.end.tv_sec - op->perf_tick.start.tv_sec) * 1000 * 1000 + \
+             (op->perf_tick.end.tv_usec - op->perf_tick.start.tv_usec);
+    } else if (p->op.op == CEPH_OSD_OP_WRITE || p->op.op == CEPH_OSD_OP_WRITEFULL) {
+      write.op_count++;
+      write.time_cost += (op->perf_tick.end.tv_sec - op->perf_tick.start.tv_sec) * 1000 * 1000 + \
+	     (op->perf_tick.end.tv_usec-op->perf_tick.start.tv_usec);
+    }
+  }
+  return;
+}
+
+std::function<void ()> ClientAdaptorPerf::create_thread(const ClientAdaptorPlugin* in) {
+  const ClientAdaptorPlugin* plugin = in;
+  return [this, plugin](){
+    char thread_name[16];
+    sprintf(thread_name, "ca-perf-tick");
+    pthread_setname_np(pthread_self(), thread_name);
+    uint64_t read_cnt = 0;
+    uint64_t read_cost = 0;
+    uint64_t write_cnt = 0;
+    uint64_t write_cost = 0;
+    uint64_t read_lat = 0xff;
+    uint64_t write_lat = 0xff;
+    float avg_flight = 0;
+
+    while (!tick_done) {
+	    sleep (3);
+	    read_cnt = plugin->perf_ref->read.op_count;
+	    read_cost = plugin->perf_ref->read.time_cost;
+	    write_cnt = plugin->perf_ref->write.op_count;
+	    write_cost = plugin->perf_ref->write.time_cost;
+	    if (read_cnt != 0) {
+		    read_lat = read_cost/read_cnt;
+	    }
+	    if (write_cnt != 0) {
+		    write_lat = write_cost/write_cnt;
+	    }
+	    if ((read_cnt + write_cnt) != 0 ) {
+		    avg_flight = (float)total_in_flight / (float)(read_cnt + write_cnt);
+	    }
+	    outfile << "*************************************************************************" << std::endl;
+	    outfile << "PID: " << getpid() << "    TID: " << gettid() << std::endl;
+	    outfile << "          total_count     avg_latency(us)" << std::endl;
+	    outfile << "read " << setw(16) << read_cnt << "    " << setw(16) << read_lat << std::endl;
+	    outfile << "write" << setw(16) << write_cnt << "    " << setw(16) << write_lat << std::endl;
+	    outfile << "          total_count      average" << std::endl;
+	    outfile << "in-flight" << setw(12) << total_in_flight
+		    <<  "   " << setw(16) << fixed << setprecision(1) << avg_flight << std::endl;
+    }
+   };
+}
+
+void ClientAdaptorPerf::start_record(ClientAdaptorPlugin* plugin) {
+	std::function<void ()> perf_thread = create_thread(plugin);
+	threads.push_back(std::thread(perf_thread));
+	outfile.open("/var/log/ceph/perf_tick.log", ios::out | ios::app);
+	return;
+}
diff --git a/src/client_adaptor/ClientAdaptorPerf.h b/src/client_adaptor/ClientAdaptorPerf.h
new file mode 100644
index 0000000..f7ee5be
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPerf.h
@@ -0,0 +1,55 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_PERF_H
+#define CLIENT_ADAPTOR_PERF_H
+
+#include <stdint.h>
+#include <sys/time.h>
+#include <thread>
+#include <iomanip>
+#include <sched.h>
+#include <vector>
+#include <iostream>
+#include <fstream>
+
+#include "osdc/Objecter.h"
+
+class ClientAdaptorPlugin;
+
+class ClientAdaptorPerf {
+public:
+  ClientAdaptorPerf(){}
+  ~ClientAdaptorPerf(){}
+
+
+  struct op_perf_t {
+    std::atomic<uint64_t> op_count{0};
+    std::atomic<uint64_t> time_cost{0};
+  };
+
+void start_tick(Objecter::Op *op);
+
+void end_tick(Objecter::Op *op);
+  
+void record_op(Objecter::Op *op); 
+
+void start_record(ClientAdaptorPlugin* plugin); 
+
+std::function<void ()> create_thread(const ClientAdaptorPlugin* plugin);
+
+const string name() {return "ClientAdaptorPerf";}
+vector<std::thread> threads;
+bool tick_done{false};
+std::ofstream outfile;
+std::atomic<uint64_t> total_in_flight{0};
+private:
+  struct op_perf_t read;
+  struct op_perf_t write;
+
+};
+
+#endif
diff --git a/src/client_adaptor/ClientAdaptorPlugin.cc b/src/client_adaptor/ClientAdaptorPlugin.cc
new file mode 100644
index 0000000..c44e348
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPlugin.cc
@@ -0,0 +1,45 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include "ClientAdaptorPlugin.h"
+#include "ceph_ver.h"
+#include "ClientAdaptorMsg.h"
+#include "ClientAdaptorMgr.h"
+#include "ClientAdaptorPerf.h"
+
+
+
+
+ClientAdaptorPlugin::~ClientAdaptorPlugin() {
+    delete mgr_ref;
+    delete msg_ref;
+    delete perf_ref;
+}
+
+const char *__ceph_plugin_version()
+{
+  return CEPH_GIT_NICE_VER;
+}
+
+
+int __ceph_plugin_init(CephContext *cct,
+		       const std::string& type,
+		       const std::string& name)
+{
+  PluginRegistry *instance = cct->get_plugin_registry();
+  if (cct->_conf.get_val<bool>("global_cache_debug_mode")){
+    ClientAdaptorLocal* ccm = new ClientAdaptorLocal();
+    ClientAdaptorMsg* msg = new ClientAdaptorMsg(ccm);
+    ClientAdaptorPerf* perf = new ClientAdaptorPerf();
+    return instance->add(type, name, new ClientAdaptorPlugin(cct, msg, ccm, perf));
+  } else {
+    ClientAdaptorCcm* ccm = new ClientAdaptorCcm();
+    ClientAdaptorMsg* msg = new ClientAdaptorMsg(ccm);
+    ClientAdaptorPerf* perf = new ClientAdaptorPerf();
+    return instance->add(type, name, new ClientAdaptorPlugin(cct, msg, ccm, perf));
+  }
+}
diff --git a/src/client_adaptor/ClientAdaptorPlugin.h b/src/client_adaptor/ClientAdaptorPlugin.h
new file mode 100644
index 0000000..fa7d71d
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPlugin.h
@@ -0,0 +1,39 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_PLUGIN_H
+#define CLIENT_ADAPTOR_PLUGIN_H
+#include <unistd.h>
+
+//#include "ceph_ver.h"
+#include "common/PluginRegistry.h"
+#include "common/ceph_context.h"
+//#include "acconfig.h"
+
+
+class ClientAdaptorMsg;
+class ClientAdaptorMgr;
+class ClientAdaptorPerf;
+
+class ClientAdaptorPlugin : public Plugin {
+public:
+  ClientAdaptorPlugin(CephContext* cct, ClientAdaptorMsg* msg, ClientAdaptorMgr* mgr,
+      ClientAdaptorPerf* perf) : Plugin(cct), msg_ref(msg), mgr_ref(mgr), perf_ref(perf)
+  {
+  }
+  
+  ~ClientAdaptorPlugin();
+
+  ClientAdaptorMsg* msg_ref;
+  ClientAdaptorMgr* mgr_ref;
+  ClientAdaptorPerf* perf_ref;
+
+  const string name() {
+    return "ClientAdaptorPlugin";
+  }
+};
+
+#endif
diff --git a/src/client_adaptor/open_ccm.h b/src/client_adaptor/open_ccm.h
new file mode 100644
index 0000000..3b43900
--- /dev/null
+++ b/src/client_adaptor/open_ccm.h
@@ -0,0 +1,431 @@
+/* 
+ * Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+*/
+#ifndef __CCM_INTERFACE_H__
+#define __CCM_INTERFACE_H__
+
+#include <stdint.h>
+#include <stdbool.h>
+#include <time.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define PT_VIEW_NODE_MAX_DOMAIN     32
+#define PT_VIEW_MAX_POOL            256
+#define PT_VIEW_MAX_COPY_NUM        6
+#define PG_VIEW_MAX_COPY_NUM        6
+#define MAX_CCM_CTRL_NODE_NUM       128
+#define PG_VIEW_COPY_NUM_3          3
+#define PG_VIEW_COPY_NUM_2          2
+#define PG_VIEW_COPY_NUM_1          1
+#define VNODE_COUNT_PER_DISK        2
+
+#define MAX_PT_ENTRY          1024
+#define MIN_PT_ENTRY          3
+#define MAX_PG_ENTRY          1024
+#define MIN_PG_ENTRY          3
+#define MAX_SERVER_NUM        MAX_CCM_CTRL_NODE_NUM
+
+#define CCM_MAX_DISK_NUM      1024
+#define CCM_VNODE_NUM_PER_NODE   8
+#define MAX_DISK_NUM_PER_NODE   16
+#define IP_ADDR_LEN           (16)
+#define DISK_NAME_LEN         (64)
+#define DISK_SN_LEN           (64)
+
+#define MAX_POOL_NAME_LEN     (256)
+#define CCM_MAX_POOL_NUM      (4096)
+#define MAX_PORT_NUM          (8)
+
+#define ZK_IP_ADDR_LEN        (16)
+#define ZK_DISK_NAME_LEN      (64)
+#define ZK_DISK_SN_LEN        (64)
+
+
+typedef enum {
+    NODE_STATE_INVALID  = 0,
+    NODE_STATE_UP       = 1,
+    NODE_STATE_STARTING = 2,
+    NODE_STATE_RUNNING  = 3,
+    NODE_STATE_UNWORK   = 4,
+    NODE_STATE_DOWN     = 5,
+    NODE_STATE_BUTT
+} NodeState;
+
+typedef enum {
+    NODE_CLUSTER_STATE_OUT  = 0,
+    NODE_CLUSTER_STATE_IN   = 1,
+    NODE_CLUSTER_STATE_BUTT
+} NodeClusterState;
+
+typedef enum {
+    VDISK_STATE_DOWN = 0,
+    VDISK_STATE_UP   = 1,
+    VIDSK_STATE_BUTT
+} VdiskState;
+
+typedef struct {
+    uint32_t NodeId;
+    uint32_t ptNum;
+    uint32_t (*ptMap)[2];
+} NodePtInfo;
+
+typedef struct {
+    uint32_t ipv4Addr;
+    uint32_t port;
+} NetInfo;
+
+typedef enum {
+    CCM_NET_CONNECTED   = 1,
+    CCM_NET_CONNECTLOOS = 2,
+    CCM_NET_BUTT
+} NetState;
+
+typedef struct {
+    uint32_t srcNid;
+    uint32_t srcNodeVersion;
+    uint32_t dstNid;
+    uint32_t dstNodeVersion;
+} NetChangeInfo;
+
+typedef struct {
+    NetChangeInfo netInfo;
+    NetState state;
+} NetChangeEvent;
+
+typedef struct {
+    uint32_t nodeId;
+    uint32_t diskId;
+    uint32_t localDiskId;
+    char diskName[DISK_NAME_LEN];
+    char sn[DISK_SN_LEN];
+    uint32_t capacity;
+    uint32_t usedCap;
+    VdiskState state;
+} VdiskInfo;
+
+typedef struct {
+    uint32_t nodeId;
+    NodeState state;
+    uint32_t ipv4addr;
+    char ipv4AddrStr[IP_ADDR_LEN];
+    char publicAddrStr[IP_ADDR_LEN];
+    char clusterAddrStr[IP_ADDR_LEN];
+    int32_t portNum;
+    uint32_t ports[MAX_PORT_NUM];
+    uint32_t diskNum;
+    VdiskInfo diskList[MAX_DISK_NUM_PER_NODE];
+    uint64_t version;
+} NodeInfo;
+
+/* ClusterInfo */
+typedef enum {
+    CLUSTER_STATE_POWER_ON  = 1,
+    CLUSTER_STATE_RUNNING   = 2,
+    CLUSTER_STATE_WARN      = 3,
+    CLUSTER_STATE_ERROR     = 4,
+    CLUSTER_STATE_OK        = 5,
+    CLUSTER_STATE_UNKNOWN   = 6
+} ClusterState;
+
+typedef struct {
+    uint32_t clusterId;
+    ClusterState clusterState;
+    uint32_t nodeNum;
+    NodeInfo nodeList[MAX_SERVER_NUM];
+    uint32_t diskNum;
+    VdiskInfo vdiskList[CCM_MAX_DISK_NUM];
+} ClusterInfo;
+
+typedef struct __NodeEventHandle {
+    int32_t (*nodeEventHandle)(void *nodeList, int32_t nodeNum);
+} NodeEventHandle;
+
+/* PGInfo & PGView */
+typedef enum __PGState {
+    CCM_PG_STATE_INIT   = 0,
+    CCM_PG_STATE_NORMAL = 1,
+    CCM_PG_STATE_DOWN   = 2,
+    CCM_PG_STATE_DEGRADE_LOSS1 = 3,
+    CCM_PG_STATE_DEGRADE_LOSS2 = 4,
+    CCM_PG_STATE_DEGRADE_LOSS3 = 5,
+    CCM_PG_STATE_DEGRADE_LOSS4 = 6,
+    CCM_PG_STATE_DEGRADE_LOSS5 = 7,
+    CCM_PG_STATE_DEGRADE_LOSS6 = 8,
+    CCM_PG_STATE_RECOVERY   = 9,
+    CCM_PG_STATE_FAULT      = 10,
+    CCM_PG_STATE_BUTT
+} PGState;
+
+/* node state of PT replicas */
+typedef enum __PgNodeState {
+    CCM_PG_NODE_STATE_UP = 1,
+    CCM_PG_NODE_STATE_DOWN = 2,
+    CCM_PG_NODE_STATE_OUT = 3,
+    CCM_PG_NODE_STATE_BUTT
+} PgNodeState;
+
+/* disk state of PT replicas */
+typedef enum __PgDiskState {
+    CCM_PG_DISK_STATE_IN = 1,
+    CCM_PG_DISK_STATE_OUT = 2,
+    CCM_PG_DISK_STATE_BUTT
+} PgDiskState;
+
+
+typedef enum __PgCopyState {
+    CCM_PG_COPY_STATE_RUNNING = 1,
+    CCM_PG_COPY_STATE_DOWN = 2,
+    CCM_PG_COPY_STATE_OUT = 3,
+    CCM_PG_COPY_STATE_RECOVERY = 4,
+    CCM_PG_COPY_STATE_BUTT
+} PgCopyState;
+
+typedef enum {
+    PG_REP_SINGLE = 1,
+    PG_REP_DOUBLE = 2,
+    PG_REP_TRIPLE = 3,
+    PG_REP_SIXPLE = 6,
+    PG_REP_STRATEGY_BUTTON
+} PGRepStrategy;
+
+typedef struct {
+    uint32_t nodeId;
+    uint32_t diskId;
+    PgCopyState state;
+    PgNodeState nodeState;
+    PgDiskState diskState;
+} PgEntryCopy;
+
+typedef struct {
+    uint32_t pgId;
+    uint64_t globalVersion;
+    uint64_t birthVersion;
+    uint32_t currCopyNum;
+    uint32_t masterNodeId;
+    uint32_t masterDiskId;
+    uint32_t activeSize;
+    uint32_t vnodeId;
+    PGState state;
+    PgEntryCopy copyList[PG_VIEW_MAX_COPY_NUM];
+} PgEntry;
+
+typedef PgEntry PgInfo;
+typedef struct __PgView PgView;
+
+struct __PgView {
+    uint32_t globalVersion;
+    uint32_t pgNum;
+    uint32_t copyNum;
+    uint8_t  reserve[8];
+    PgEntry pgInfo[0];
+};
+
+/* PtView & PtInfo*/
+typedef enum {
+    CCM_PT_STATE_INIT=0,
+    CCM_PT_STATE_OK,
+    CCM_PT_STATE_TRIM,
+    CCM_PT_STATE_REPLAY,
+    CCM_PT_STATE_FAULT,
+} PtState;
+
+typedef enum {
+    CCM_PT_SPLIT_RUNNING=0,
+    CCM_PT_SPLIT_COMPLETED,
+    CCM_PT_SPLIT_STOP,
+    CCM_PT_SPLIT_NONE,
+} SplitState;
+
+typedef struct {
+    uint32_t nodeID;
+    uint32_t vdiskID;
+    uint32_t vnode;
+} PtPeerInfo;
+
+typedef struct {
+    uint32_t nodeId;
+    uint32_t vnodeId;
+} PtSrcNodeInfo;
+
+typedef struct {
+    bool ptChange;
+    uint32_t birthVersion;
+    uint32_t ptId;
+    uint32_t indexInNode;
+    uint32_t masterNode;
+    PtState state;
+    SplitState splitState;
+    bool isRecoverd;
+    PtPeerInfo peerGroup[2];
+    PtPeerInfo targetPeerGroup[2];
+    PtSrcNodeInfo srcNodeInfo;
+} PtInfo;
+
+typedef struct _PtView {
+    uint32_t globalVersion;
+    uint32_t ptNum;
+    uint8_t reserve[8];
+    PtInfo ptInfo[0];
+} PtView;
+
+typedef PtInfo PTViewPtEntry;
+
+typedef struct {
+    void (*handle)(void *arg);
+    void *arg;
+} DeleteOpHandler;
+
+/* PGView callback */
+typedef struct {
+    void *ctx;
+    int32_t (*notifyPgChange)(PgEntry *entry, uint32_t entryNum, void *ctx);
+} PGViewChangeOpHandle;
+
+/* NodeView callback */
+typedef struct {
+    void *ctx;
+    int32_t (*notifyNodeChange)(NodeInfo *nodeList, uint32_t nodeNum, void *ctx);
+} NodeViewChangeOpHandle;
+
+/* PTView callback */
+typedef struct {
+    void *ctx;
+    int32_t (*notifyPtChange)(PTViewPtEntry *entry, uint32_t entryNum, void *ctx);
+} PTViewChangeOpHandle;
+
+/* PG recovery callback */
+typedef struct {
+    void *ctx;
+    int32_t (*recovery)(PgView *pgView, void *ctx);
+} PGRecoverHandle;
+
+/* PT Split callback */
+typedef struct {
+    void *ctx;
+    int32_t (*split)(PtView *ptView, void *ctx);
+} PTSplitHandle;
+
+/* PT Trim callback */
+typedef struct {
+    void *ctx;
+    int32_t (*trim)(PTViewPtEntry *entry, uint32_t entryNum, void *ctx);
+} PTTrimHandle;
+
+/* PT Recovery callback */
+typedef struct {
+    void *ctx;
+    int32_t (*replay)(PTViewPtEntry *entry, uint32_t entryNum, void *ctx);
+} PTReplayHandle;
+
+typedef struct {
+    void *ctx;
+    int32_t (*powerOn)(void *ctx);
+} PowerOnHandle;
+
+typedef enum {
+    CCM_MODULE_INFRAS = 0,
+    CCM_MODULE_PLOG,
+    CCM_MODULE_INDEX,
+    CCM_MODULE_CACHE,
+    CCM_MODULE_CLIENT,
+    CCM_MODULE_CEPH,
+    CCM_MODULE_BUTT,
+} ModuleType;
+
+typedef enum {
+    PT = 0,
+    PG = 1,
+} EntryType;
+
+typedef enum {
+    RECOVERY_OK = 1,
+    RECOVERY_FAILD,
+
+    SPLIT_OK,
+    SPLIT_FAILD,
+    SPLIT_AGAIN,
+
+    TRIM_OK,
+    TRIM_FAILD,
+
+    REPLAY_OK,
+    REPLAY_FAILD,
+} PeerEventType;
+
+typedef struct {
+    uint32_t id;
+    uint32_t version;
+    uint32_t nodeId;
+    PeerEventType type;
+} PeerEntryInfo;
+
+typedef enum {
+    CCM_MODULE_STATE_INITING    = 0,
+    CCM_MODULE_STATE_REPLAYING  = 1,
+    CCM_MODULE_STATE_REPLAYED   = 2,
+    CCM_MODULE_STATE_RECOVERING = 3,
+    CCM_MODULE_STATE_DOWN       = 4,
+    CCM_MODULE_STATE_BUTT,
+} ModuleState;
+
+typedef enum {
+    CACHENODE = 0,
+    CLIENTNODE = 1,
+} NodeType;
+
+typedef enum {
+    IOTYPE_READ = 0,
+    IOTYPE_WRITE = 1,
+} IOType_E;
+
+typedef struct {
+    uint64_t ioCount;
+    uint64_t readCount;
+    uint64_t readSize;
+    uint64_t writeCount;
+    uint64_t writeSize;
+} PtIOStat;
+
+typedef struct {
+    uint64_t ptNum;
+    PtIOStat table[MAX_PT_ENTRY];
+} PtViewIOStat;
+
+int32_t OpenAgentInit(bool isServer);
+
+int32_t OpenGetPtEntry(uint32_t ptId, PTViewPtEntry *entry);
+
+uint32_t OpenGetTotalPtNum(void);
+
+int32_t OpenAgentGetNodeInfo(uint32_t nodeId, NodeInfo *nodeInfo);
+
+int32_t OpenRegisterViewChangeNotifyChain(PTViewChangeOpHandle *handle, uint8_t type);
+
+void OpenDeregisterViewChangeNotifyChain(PTViewChangeOpHandle *handle);
+
+int32_t OpenCreateSnapshot(const int64_t poolId, const char *imageId, const uint64_t snapId);
+
+int32_t OpenDeleteSnapshot(const int64_t poolId, const char *imageId, const uint64_t snapId);
+
+int32_t OpenReleaseImageResource(const int64_t poolId, const char *imageId);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif // __CCM_INTERFACE_H__
\ No newline at end of file
diff --git a/src/client_adaptor/open_das.h b/src/client_adaptor/open_das.h
new file mode 100644
index 0000000..ccb4548
--- /dev/null
+++ b/src/client_adaptor/open_das.h
@@ -0,0 +1,66 @@
+/* 
+ * Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+*/
+#ifndef OPEN_DAS_H
+#define OPEN_DAS_H
+
+#include <cstdio>
+#include <cstdint>
+#include <cstdlib>
+#include <string>
+
+typedef enum EnumDasResult {
+    RETURN_DAS_FULL = -2,
+    RETURN_DAS_ERROR = -1,
+    RETURN_DAS_OK = 0,
+    RETURN_DAS_EMPTY = 1,
+    RETURN_DAS_DELETING = 2,
+} DAS_RESULT;
+
+typedef enum TagDasAlgType {
+    DAS_ALG_SEQ = 0,
+    DAS_ALG_REVERSE_SEQ,
+    DAS_ALG_STRIDE,
+    DAS_ALG_BUTT,
+} DasAlgType;
+
+typedef struct TagDasKvParam {
+    uint64_t offset;
+    uint64_t len;
+    uint8_t opcode;
+    uint64_t timeStamp;
+    uint64_t cephPoolId;
+    DasAlgType algType;
+    uint64_t objId;
+    uint32_t imageIdLen;
+    void *ctx;
+    void *handle;
+    char imageIdBuf[0];
+} DasKvParam;
+
+typedef struct TagDasOPS {
+    void (*SubmitDasPrefetch)(DasKvParam* params);
+} DasOPS;
+
+typedef struct TagDasModuleParam {
+    DasOPS *ops;
+} DasModuleParam;
+
+int32_t OpenRcacheCeateDasModule(void *handle, DasModuleParam *createInstanceParam);
+
+int32_t OpenRcachePutDasInfo(DasKvParam *params[], uint32_t keyNum);
+
+void OpenRcacheExitDasModule(void *handle);
+#endif // OPEN_DAS_H
\ No newline at end of file
diff --git a/src/common/options.cc b/src/common/options.cc
index 8135ea8..9d3bb78 100644
--- a/src/common/options.cc
+++ b/src/common/options.cc
@@ -1015,7 +1015,7 @@ std::vector<Option> get_global_options() {
     .set_description("Induce a crash/exit on various bugs (for testing purposes)"),
 
     Option("ms_dispatch_throttle_bytes", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
-    .set_default(100_M)
+    .set_default(2048_M)
     .set_description("Limit messages that are read off the network but still being processed"),
 
     Option("ms_bind_ipv4", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
@@ -2308,6 +2308,15 @@ std::vector<Option> get_global_options() {
     .set_default(10.0)
     .set_description("Seconds before in-flight op is considered 'laggy' and we query mon for the latest OSDMap"),
 
+#ifdef WITH_GLOBAL_CACHE
+    Option("objecter_inflight_op_bytes", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
+    .set_default(0)
+    .set_description("Max in-flight data in bytes (both directions)"),
+
+    Option("objecter_inflight_ops", Option::TYPE_UINT, Option::LEVEL_ADVANCED)
+    .set_default(0)
+    .set_description("Max in-flight operations"),
+#else
     Option("objecter_inflight_op_bytes", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
     .set_default(100_M)
     .set_description("Max in-flight data in bytes (both directions)"),
@@ -2315,6 +2324,7 @@ std::vector<Option> get_global_options() {
     Option("objecter_inflight_ops", Option::TYPE_UINT, Option::LEVEL_ADVANCED)
     .set_default(1024)
     .set_description("Max in-flight operations"),
+#endif
 
     Option("objecter_completion_locks_per_session", Option::TYPE_UINT, Option::LEVEL_DEV)
     .set_default(32)
@@ -5587,6 +5597,14 @@ std::vector<Option> get_global_options() {
     Option("debug_heartbeat_testing_span", Option::TYPE_INT, Option::LEVEL_DEV)
     .set_default(0)
     .set_description("Override 60 second periods for testing only"),
+#ifdef WITH_GLOBAL_CACHE
+    Option("global_cache_debug_mode", Option::TYPE_BOOL, Option::LEVEL_DEV)
+    .set_default(false)
+    .set_description("Global Cache client adaptor local debug mode switch"),
+    Option("global_cache_tick", Option::TYPE_BOOL, Option::LEVEL_DEV)
+    .set_default(false)
+    .set_description("Global Cache client adaptor performance tick switch"),
+#endif
   });
 }
 
@@ -7222,11 +7240,15 @@ static std::vector<Option> get_rbd_options() {
     Option("rbd_non_blocking_aio", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
     .set_default(true)
     .set_description("process AIO ops from a dispatch thread to prevent blocking"),
-
+#ifdef WITH_GLOBAL_CACHE
+    Option("rbd_cache", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
+    .set_default(false)
+    .set_description("whether to enable caching (writeback unless rbd_cache_max_dirty is 0)"),
+#else
     Option("rbd_cache", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
     .set_default(true)
     .set_description("whether to enable caching (writeback unless rbd_cache_max_dirty is 0)"),
-
+#endif
     Option("rbd_cache_writethrough_until_flush", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
     .set_default(true)
     .set_description("whether to make writeback caching writethrough until "
@@ -7537,6 +7559,15 @@ static std::vector<Option> get_rbd_options() {
     .set_default(60)
     .set_min(0)
     .set_description("RBD Image access timestamp refresh interval. Set to 0 to disable access timestamp update."),
+#ifdef WITH_GLOBAL_CACHE
+    Option("global_cache", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
+    .set_default(true)
+    .set_description("whether to enable global cache"),
+
+    Option("gc_perf", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
+    .set_default(true)
+    .set_description("whether to enable global cache perf"),
+#endif
   });
 }
 
diff --git a/src/include/config-h.in.cmake b/src/include/config-h.in.cmake
index d83a59b..12dfdf5 100644
--- a/src/include/config-h.in.cmake
+++ b/src/include/config-h.in.cmake
@@ -187,6 +187,9 @@
 /* Define if you want to use Babeltrace */
 #cmakedefine WITH_BABELTRACE
 
+/* Define if you want to use Global Cache */
+#cmakedefine WITH_GLOBAL_CACHE
+
 /* Define to 1 if you have the <babeltrace/babeltrace.h> header file. */
 #cmakedefine HAVE_BABELTRACE_BABELTRACE_H 1
 
diff --git a/src/librbd/ImageCtx.cc b/src/librbd/ImageCtx.cc
index 8375d1a..f79941e 100644
--- a/src/librbd/ImageCtx.cc
+++ b/src/librbd/ImageCtx.cc
@@ -270,6 +270,18 @@ public:
                         "wb", perf_prio, unit_t(UNIT_BYTES));
     plb.add_time_avg(l_librbd_wr_latency, "wr_latency", "Write latency",
                      "wl", perf_prio);
+#ifdef WITH_GLOBAL_CACHE
+    plb.add_time_avg(l_librbd_rd_before_queue_op_lat, "rd_before_queue_latency", "before queue latency",
+		    "rbql", perf_prio);
+    plb.add_time_avg(l_librbd_wr_before_queue_op_lat, "wr_before_queue_latency", "before queue latency",
+		    "wbql", perf_prio);
+    plb.add_time_avg(l_librbd_after_dequeue_op_lat, "after_dequeue_latency", "after dequeue latency",
+		    "adl", perf_prio);
+    plb.add_time_avg(l_librbd_send_lat, "send_latency", "send latency",
+		    "send", perf_prio);
+    plb.add_u64(l_librbd_rd_queue, "rqueue", "q", "q", PerfCountersBuilder::PRIO_USEFUL);
+    plb.add_u64(l_librbd_wr_queue, "wqueue", "q", "q", PerfCountersBuilder::PRIO_USEFUL);
+#endif
     plb.add_u64_counter(l_librbd_discard, "discard", "Discards");
     plb.add_u64_counter(l_librbd_discard_bytes, "discard_bytes", "Discarded data", NULL, 0, unit_t(UNIT_BYTES));
     plb.add_time_avg(l_librbd_discard_latency, "discard_latency", "Discard latency");
@@ -778,9 +790,7 @@ public:
 
     bool skip_partial_discard = true;
     ASSIGN_OPTION(non_blocking_aio, bool);
-    ASSIGN_OPTION(cache, bool);
     ASSIGN_OPTION(cache_writethrough_until_flush, bool);
-    ASSIGN_OPTION(cache_max_dirty, Option::size_t);
     ASSIGN_OPTION(sparse_read_threshold_bytes, Option::size_t);
     ASSIGN_OPTION(readahead_max_bytes, Option::size_t);
     ASSIGN_OPTION(readahead_disable_after_bytes, Option::size_t);
diff --git a/src/librbd/ImageCtx.h b/src/librbd/ImageCtx.h
index df5271e..2e8d7f0 100644
--- a/src/librbd/ImageCtx.h
+++ b/src/librbd/ImageCtx.h
@@ -174,9 +174,9 @@ namespace librbd {
 
     /// Cached latency-sensitive configuration settings
     bool non_blocking_aio;
-    bool cache;
+    bool cache = false; // bypass rbd cache feature
     bool cache_writethrough_until_flush;
-    uint64_t cache_max_dirty;
+    uint64_t cache_max_dirty = 0;
     uint64_t sparse_read_threshold_bytes;
     uint64_t readahead_max_bytes;
     uint64_t readahead_disable_after_bytes;
diff --git a/src/librbd/Types.h b/src/librbd/Types.h
index 3f11044..e33023f 100644
--- a/src/librbd/Types.h
+++ b/src/librbd/Types.h
@@ -22,6 +22,14 @@ enum {
   l_librbd_wr,
   l_librbd_wr_bytes,
   l_librbd_wr_latency,
+#ifdef WITH_GLOBAL_CACHE
+  l_librbd_rd_before_queue_op_lat,
+  l_librbd_wr_before_queue_op_lat,
+  l_librbd_after_dequeue_op_lat,
+  l_librbd_send_lat,
+  l_librbd_wr_queue,
+  l_librbd_rd_queue,
+#endif
   l_librbd_discard,
   l_librbd_discard_bytes,
   l_librbd_discard_latency,
diff --git a/src/librbd/image/RemoveRequest.cc b/src/librbd/image/RemoveRequest.cc
index 8e029f8..85b06e2 100644
--- a/src/librbd/image/RemoveRequest.cc
+++ b/src/librbd/image/RemoveRequest.cc
@@ -14,6 +14,10 @@
 #include "librbd/journal/RemoveRequest.h"
 #include "librbd/mirror/DisableRequest.h"
 #include "librbd/operation/TrimRequest.h"
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#endif
 
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
@@ -58,6 +62,15 @@ template<typename I>
 void RemoveRequest<I>::send() {
   ldout(m_cct, 20) << dendl;
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = m_cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  if (plugin == NULL) {
+    ldout(m_cct, 5) << "global_cache plugin not loaded" << dendl;
+    finish(-ELIBACC);
+    return;
+  }
+#endif
   open_image();
 }
 
@@ -393,6 +406,9 @@ void RemoveRequest<I>::handle_mirror_image_remove(int r) {
   if (m_from_trash_remove) {
     // both the id object and the directory entry have been removed in
     // a previous call to trash_move.
+#ifdef WITH_GLOBAL_CACHE
+    remove_from_gc();
+#endif
     finish(0);
     return;
   }
@@ -433,7 +449,9 @@ void RemoveRequest<I>::handle_remove_v1_image(int r) {
       lderr(m_cct) << "error removing image from v1 directory: "
                    << cpp_strerror(r) << dendl;
     }
-
+#ifdef WITH_GLOBAL_CACHE
+    remove_from_gc();
+#endif
     m_on_finish->complete(r);
     delete this;
     return;
@@ -587,10 +605,32 @@ void RemoveRequest<I>::handle_dir_remove_image(int r) {
     lderr(m_cct) << "error removing image from v2 directory: "
                  << cpp_strerror(r) << dendl;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  remove_from_gc();
+#endif
   finish(r);
 }
 
+#ifdef WITH_GLOBAL_CACHE
+template<typename I>
+void RemoveRequest<I>::remove_from_gc() {
+    int64_t pool_id = m_ioctx.get_id();
+    ldout(m_cct, 20) << "pool_id=" << pool_id
+                    << " image_id=" << m_image_id
+                     << " image_name=" << m_image_name
+                     << dendl;
+    PluginRegistry *reg = m_cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    int64_t ret = plugin->mgr_ref->remove_gc_image_resource(pool_id, m_image_id);
+    if (ret == 0) {
+        ldout(m_cct, 20) << "remove image from gc successfully, image_id=" << m_image_id << dendl;
+    } else {
+        ldout(m_cct, 5) <<"remove image from gc failed, image_id=" << m_image_id << " ret=" << ret << dendl;
+    }
+}
+#endif
+
 template<typename I>
 void RemoveRequest<I>::finish(int r) {
   ldout(m_cct, 20) << "r=" << r << dendl;
diff --git a/src/librbd/image/RemoveRequest.h b/src/librbd/image/RemoveRequest.h
index 98d5976..b92af2d 100644
--- a/src/librbd/image/RemoveRequest.h
+++ b/src/librbd/image/RemoveRequest.h
@@ -96,6 +96,10 @@ private:
    * |               |                /  |
    * |               \-------<-------/   |
    * |                                   v
+   * |                            REMOVE GC SNAP INFO
+   * |                                   |
+   * |                                   |
+   * |                                   v
    * \------------------>------------<finish>
    *
    * @endverbatim
@@ -187,6 +191,10 @@ private:
   void dir_remove_image();
   void handle_dir_remove_image(int r);
 
+#ifdef WITH_GLOBAL_CACHE
+  void remove_from_gc();
+#endif
+
   void finish(int r);
 };
 
diff --git a/src/librbd/io/ImageDispatchSpec.h b/src/librbd/io/ImageDispatchSpec.h
index 93c53a0..7551fb0 100644
--- a/src/librbd/io/ImageDispatchSpec.h
+++ b/src/librbd/io/ImageDispatchSpec.h
@@ -121,7 +121,11 @@ public:
     return new ImageDispatchSpec(image_ctx, aio_comp, {}, Flush{flush_source},
                                  0, parent_trace);
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  AioCompletion* get_completion() {
+    return m_aio_comp;
+  }
+#endif
   void send();
   void fail(int r);
 
diff --git a/src/librbd/io/ImageRequestWQ.cc b/src/librbd/io/ImageRequestWQ.cc
index 34f2c2c..cae0341 100644
--- a/src/librbd/io/ImageRequestWQ.cc
+++ b/src/librbd/io/ImageRequestWQ.cc
@@ -286,7 +286,13 @@ void ImageRequestWQ<I>::aio_read(AioCompletion *c, uint64_t off, uint64_t len,
   RWLock::RLocker owner_locker(m_image_ctx.owner_lock);
   if (m_image_ctx.non_blocking_aio || writes_blocked() || !writes_empty() ||
       require_lock_on_read()) {
+#ifdef WITH_GLOBAL_CACHE
+    ceph::timespan elapsed = coarse_mono_clock::now() - c->start_time;
+    m_image_ctx.perfcounter->tinc(l_librbd_rd_before_queue_op_lat, elapsed);
     queue(ImageDispatchSpec<I>::create_read_request(
+#else
+    queue(ImageDispatchSpec<I>::create_read_request(
+#endif
             m_image_ctx, c, {{off, len}}, std::move(read_result), op_flags,
             trace));
   } else {
@@ -325,7 +331,13 @@ void ImageRequestWQ<I>::aio_write(AioCompletion *c, uint64_t off, uint64_t len,
 
   RWLock::RLocker owner_locker(m_image_ctx.owner_lock);
   if (m_image_ctx.non_blocking_aio || writes_blocked()) {
+#ifdef WITH_GLOBAL_CACHE
+    ceph::timespan elapsed = coarse_mono_clock::now() - c->start_time;
+    m_image_ctx.perfcounter->tinc(l_librbd_wr_before_queue_op_lat, elapsed);
+    queue(ImageDispatchSpec<I>::create_write_request(
+#else
     queue(ImageDispatchSpec<I>::create_write_request(
+#endif
             m_image_ctx, c, {{off, len}}, std::move(bl), op_flags, trace));
   } else {
     c->start_op();
@@ -790,8 +802,16 @@ void ImageRequestWQ<I>::process(ImageDispatchSpec<I> *req) {
   CephContext *cct = m_image_ctx.cct;
   ldout(cct, 20) << "ictx=" << &m_image_ctx << ", "
                  << "req=" << req << dendl;
-
+#ifdef WITH_GLOBAL_CACHE
+  ceph::timespan elapsed = coarse_mono_clock::now() - req->get_completion()->start_time;
+  m_image_ctx.perfcounter->tinc(l_librbd_after_dequeue_op_lat, elapsed);
+  coarse_mono_time before_send = coarse_mono_clock::now();
+#endif
   req->send();
+#ifdef WITH_GLOBAL_CACHE
+  ceph::timespan send_time = coarse_mono_clock::now() - before_send;
+  m_image_ctx.perfcounter->tinc(l_librbd_send_lat, send_time);
+#endif
 
   finish_queued_io(req);
   if (req->is_write_op()) {
@@ -905,6 +925,10 @@ void ImageRequestWQ<I>::queue(ImageDispatchSpec<I> *req) {
   } else {
     m_queued_reads++;
   }
+#ifdef WITH_GLOBAL_CACHE
+  m_image_ctx.perfcounter->set(l_librbd_rd_queue, m_queued_reads);
+  m_image_ctx.perfcounter->set(l_librbd_wr_queue, m_queued_writes);
+#endif
 
   ThreadPool::PointerWQ<ImageDispatchSpec<I> >::queue(req);
 }
diff --git a/src/librbd/operation/SnapshotCreateRequest.cc b/src/librbd/operation/SnapshotCreateRequest.cc
index 6629360..ba441b7 100644
--- a/src/librbd/operation/SnapshotCreateRequest.cc
+++ b/src/librbd/operation/SnapshotCreateRequest.cc
@@ -11,6 +11,11 @@
 #include "librbd/Utils.h"
 #include "librbd/io/ImageRequestWQ.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd::SnapshotCreateRequest: "
@@ -45,6 +50,16 @@ void SnapshotCreateRequest<I>::send_op() {
     return;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  if (plugin == NULL) {
+      lderr(cct) << "client adaptor plugin not found" << dendl;
+      this->async_complete(-ELIBACC);
+      return;
+  }
+#endif
+
   send_suspend_requests();
 }
 
@@ -210,9 +225,55 @@ Context *SnapshotCreateRequest<I>::handle_create_snap(int *result) {
     return nullptr;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  return send_add_snap_to_gc();
+#else
   return send_create_object_map();
+#endif
 }
 
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+Context *SnapshotCreateRequest<I>::send_add_snap_to_gc() {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    int64_t pool_id = image_ctx.md_ctx.get_id();
+    std::string pool_name;
+    ldout(cct, 5) << this << " Client Adaptor: " << __func__
+                                                   << " pool_id: " << pool_id
+                                                   << " image_id: " << image_ctx.id
+                                                   << " snap_id: " << m_snap_id
+                                                   << dendl;
+
+    librados::Rados rados(image_ctx.md_ctx);
+    int ret = rados.pool_reverse_lookup(pool_id, &pool_name);
+    ceph_assert(ret == 0 && !pool_name.empty());
+
+    ldout(cct, 5) << this << " Client Adaptor: " << __func__
+                                                   << " pool_name: " << pool_name
+                                                   << " image_name: " << image_ctx.name
+                                                   << " snap_name: " << m_snap_name
+                                                   << dendl;
+    
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    int mgr_ret = -ELIBACC;
+
+    if (plugin && plugin->mgr_ref) {
+        mgr_ret = plugin->mgr_ref->add_snap_to_gc(pool_id, image_ctx.id, m_snap_id);
+    }
+
+    if (mgr_ret < 0) {
+        lderr(cct) << __func__ << " Client Adaptor: " << "gc create snapshot failed" << dendl;
+        save_result(&mgr_ret);
+        send_release_snap();
+        return nullptr;
+    }
+    ldout(cct, 20) << __func__ << " Client Adaptor: " << "gc create snapshot success" << dendl;
+    return send_create_object_map();
+}
+#endif
+
 template <typename I>
 Context *SnapshotCreateRequest<I>::send_create_object_map() {
   I &image_ctx = this->m_image_ctx;
@@ -257,6 +318,43 @@ Context *SnapshotCreateRequest<I>::handle_create_object_map(int *result) {
   return this->create_context_finisher(0);
 }
 
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+void SnapshotCreateRequest<I>::send_release_snap() {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << dendl;
+
+    librados::ObjectWriteOperation op;
+    if (image_ctx.old_format) {
+        cls_client::old_snapshot_remove(&op, m_snap_name);
+    } else {
+        cls_client::snapshot_remove(&op, m_snap_id);
+    }
+
+    librados::AioCompletion *rados_completion = create_rados_callback<
+        SnapshotCreateRequest<I>,
+        &SnapshotCreateRequest<I>::handle_release_snap>(this);
+    int r = image_ctx.md_ctx.aio_operate(image_ctx.header_oid,
+                                        rados_completion, &op);
+    ceph_assert(r == 0);
+    rados_completion->release();
+}
+
+template <typename I>
+Context *SnapshotCreateRequest<I>::handle_release_snap(int *result) {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << ": r=" << *result << dendl;
+
+    ceph_assert(m_ret_val < 0);
+    *result = m_ret_val;
+
+    send_release_snap_id();
+    return nullptr;
+}
+#endif
+
 template <typename I>
 void SnapshotCreateRequest<I>::send_release_snap_id() {
   I &image_ctx = this->m_image_ctx;
diff --git a/src/librbd/operation/SnapshotCreateRequest.h b/src/librbd/operation/SnapshotCreateRequest.h
index 406d2f0..5d1dfdd 100644
--- a/src/librbd/operation/SnapshotCreateRequest.h
+++ b/src/librbd/operation/SnapshotCreateRequest.h
@@ -31,26 +31,32 @@ public:
    *           STATE_SUSPEND_REQUESTS
    *               |
    *               v
-   *           STATE_SUSPEND_AIO * * * * * * * * * * * * *
-   *               |                                     *
-   *               v                                     *
-   *           STATE_APPEND_OP_EVENT (skip if journal    *
-   *               |                  disabled)          *
-   *   (retry)     v                                     *
-   *   . . . > STATE_ALLOCATE_SNAP_ID                    *
-   *   .           |                                     *
-   *   .           v                                     *
-   *   . . . . STATE_CREATE_SNAP * * * * * * * * * *     *
-   *               |                               *     *
-   *               v                               *     *
-   *           STATE_CREATE_OBJECT_MAP (skip if    *     *
-   *               |                    disabled)  *     *
-   *               |                               *     *
-   *               |                               v     *
-   *               |              STATE_RELEASE_SNAP_ID  *
-   *               |                     |               *
-   *               |                     v               *
-   *               \----------------> <finish> < * * * * *
+   *           STATE_SUSPEND_AIO * * * * * * * * * * * * * * *
+   *               |                                         *
+   *               v                                         *
+   *           STATE_APPEND_OP_EVENT (skip if journal        *
+   *               |                  disabled)              *
+   *   (retry)     v                                         *
+   *   . . . > STATE_ALLOCATE_SNAP_ID                        *
+   *   .           |                                         *
+   *   .           v                                         *
+   *   . . . . STATE_CREATE_SNAP * * * * * * * * * * * *     *
+   *               |                                   *     *
+   *               v                                   *     *
+   *           STATE_ADD_SNAP_TO_GC  (skip if no       *     *
+   *               |                  gc)  * * * * *   *     *
+   *               v                               *   *     *
+   *           STATE_CREATE_OBJECT_MAP (skip if    *   *     *
+   *               |                    disabled)  *   *     *
+   *               |                               *   *     *
+   *               |                               v   *     *
+   *               |                STATE_RELEASE_SNAP *     *
+   *               |                               *   *     *
+   *               |                               v   v     *
+   *               |                   STATE_RELEASE_SNAP_ID *
+   *               |                     |                   *
+   *               |                     v                   *
+   *               \----------------> <finish> < * * * * * * *
    *
    * @endverbatim
    *
@@ -103,9 +109,18 @@ private:
   void send_create_snap();
   Context *handle_create_snap(int *result);
 
+#ifdef WITH_GLOBAL_CACHE
+  Context *send_add_snap_to_gc();
+#endif
+
   Context *send_create_object_map();
   Context *handle_create_object_map(int *result);
 
+#ifdef WITH_GLOBAL_CACHE
+  void send_release_snap();
+  Context *handle_release_snap(int *result);
+#endif
+
   void send_release_snap_id();
   Context *handle_release_snap_id(int *result);
 
diff --git a/src/librbd/operation/SnapshotRemoveRequest.cc b/src/librbd/operation/SnapshotRemoveRequest.cc
index 69fbc3e..a9cba36 100644
--- a/src/librbd/operation/SnapshotRemoveRequest.cc
+++ b/src/librbd/operation/SnapshotRemoveRequest.cc
@@ -11,6 +11,11 @@
 #include "librbd/Utils.h"
 #include "librbd/image/DetachChildRequest.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd::SnapshotRemoveRequest: " << this << " " \
@@ -47,6 +52,16 @@ void SnapshotRemoveRequest<I>::send_op() {
     }
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  if (plugin == NULL) {
+    lderr(cct) << "client adaptor plugin not loaded" << dendl;
+    this->async_complete(-ELIBACC);
+    return;
+  }
+#endif
+
   trash_snap();
 }
 
@@ -423,9 +438,56 @@ void SnapshotRemoveRequest<I>::handle_remove_snap(int r) {
     return;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  int32_t ret = remove_snap_from_gc();
+#endif
   remove_snap_context();
+#ifdef WITH_GLOBAL_CACHE
+  this->complete(ret);
+#else
   this->complete(0);
+#endif
+}
+
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+int32_t SnapshotRemoveRequest<I>::remove_snap_from_gc() {
+  I &image_ctx = this->m_image_ctx;
+  int64_t pool_id = image_ctx.md_ctx.get_id();
+  std::string pool_name;
+  CephContext *cct = image_ctx.cct;
+  ldout(cct, 10) << this << " Client Adaptor: " << __func__
+                                                  << " pool_id: " << pool_id
+                                                  << " image_id: " << image_ctx.id
+                                                  << " snap_id: " << m_snap_id
+                                                  << dendl;
+
+  librados::Rados rados(image_ctx.md_ctx);
+  int ret = rados.pool_reverse_lookup(pool_id, &pool_name);
+  ceph_assert(ret == 0 && !pool_name.empty());
+
+  ldout(cct, 10) << this << " Client Adaptor: " << __func__
+                                                 << " pool_name: " << pool_name
+                                                 << " image_name: " << image_ctx.name
+                                                 << " snap_name: " << m_snap_name
+                                                 << dendl;
+  
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  int mgr_ret = -ELIBACC;
+
+  if (plugin && plugin->mgr_ref) {
+      mgr_ret = plugin->mgr_ref->remove_snap_from_gc(pool_id, image_ctx.id, m_snap_id);
+  }
+
+  if (mgr_ret < 0) {
+      lderr(cct) << " Client Adaptor: " << "gc remove snapshot fail, ret=" << mgr_ret << dendl;
+  } else {
+      ldout(cct, 20) << " Client Adaptor: " << "gc remove snapshot success" << dendl;
+  }
+  return mgr_ret;
 }
+#endif
 
 template <typename I>
 void SnapshotRemoveRequest<I>::remove_snap_context() {
diff --git a/src/librbd/operation/SnapshotRemoveRequest.h b/src/librbd/operation/SnapshotRemoveRequest.h
index a47bbc0..7bab65d 100644
--- a/src/librbd/operation/SnapshotRemoveRequest.h
+++ b/src/librbd/operation/SnapshotRemoveRequest.h
@@ -49,6 +49,9 @@ public:
    *    v (skip if in-use)
    * REMOVE_SNAP
    *    |
+   *    v (skip if no gc)
+   * REMOVE_SNAP_FROM_GC
+   *    |
    *    v
    * <finish>
    *
@@ -109,6 +112,10 @@ private:
   void remove_snap();
   void handle_remove_snap(int r);
 
+#ifdef WITH_GLOBAL_CACHE
+  int32_t remove_snap_from_gc();
+#endif
+
   void remove_snap_context();
   int scan_for_parents(cls::rbd::ParentImageSpec &pspec);
 
diff --git a/src/msg/Message.cc b/src/msg/Message.cc
index d36a95e..e5384c3 100644
--- a/src/msg/Message.cc
+++ b/src/msg/Message.cc
@@ -202,6 +202,8 @@
 #include "messages/MOSDPGUpdateLogMissing.h"
 #include "messages/MOSDPGUpdateLogMissingReply.h"
 
+#include "msg/Messenger.h"
+
 #define DEBUGLVL  10    // debug level of output
 
 #define dout_subsys ceph_subsys_ms
@@ -932,12 +934,12 @@ void Message::decode_trace(bufferlist::const_iterator &p, bool create)
   const auto msgr = connection->get_messenger();
   const auto endpoint = msgr->get_trace_endpoint();
   if (info.trace_id) {
-    trace.init(get_type_name(), endpoint, &info, true);
+    trace.init(get_type_name().data(), endpoint, &info, true);
     trace.event("decoded trace");
   } else if (create || (msgr->get_myname().is_osd() &&
                         msgr->cct->_conf->osd_blkin_trace_all)) {
     // create a trace even if we didn't get one on the wire
-    trace.init(get_type_name(), endpoint);
+    trace.init(get_type_name().data(), endpoint);
     trace.event("created trace");
   }
   trace.keyval("tid", get_tid());
diff --git a/src/msg/async/AsyncConnection.cc b/src/msg/async/AsyncConnection.cc
index cab4145..98a69dd 100644
--- a/src/msg/async/AsyncConnection.cc
+++ b/src/msg/async/AsyncConnection.cc
@@ -360,8 +360,6 @@ void AsyncConnection::process() {
   last_active = ceph::coarse_mono_clock::now();
   recv_start_time = ceph::mono_clock::now();
 
-  ldout(async_msgr->cct, 20) << __func__ << dendl;
-
   switch (state) {
     case STATE_NONE: {
       ldout(async_msgr->cct, 20) << __func__ << " enter none state" << dendl;
@@ -758,12 +756,30 @@ void AsyncConnection::tick(uint64_t id)
   } else {
     auto idle_period = std::chrono::duration_cast<std::chrono::microseconds>
       (now - last_active).count();
+
     if (inactive_timeout_us < (uint64_t)idle_period) {
+#ifdef WITH_GLOBAL_CACHE
+      int con_port = target_addr.get_port();
+      if (con_port >= lower_port && con_port <= upper_port) {
+        ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
+                                << ") for more than " << inactive_timeout_us
+                                << " us, keep establish."
+                                << dendl;
+        last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
+      } else {
+        ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
+                                << ") for more than " << inactive_timeout_us
+                                << " us, fault."
+                                << dendl;
+        protocol->fault();
+      }
+#else
       ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
                                 << ") for more than " << inactive_timeout_us
                                 << " us, fault."
                                 << dendl;
       protocol->fault();
+#endif
     } else {
       last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
     }
diff --git a/src/msg/async/AsyncConnection.h b/src/msg/async/AsyncConnection.h
index 5b914cc..301cc89 100644
--- a/src/msg/async/AsyncConnection.h
+++ b/src/msg/async/AsyncConnection.h
@@ -136,6 +136,16 @@ class AsyncConnection : public Connection {
     return target_addr;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  uint32_t get_port_lower_boundary() const {
+    return lower_port;
+  }
+
+  uint32_t get_port_upper_boundary() const {
+    return upper_port;
+  }
+#endif
+
   int get_con_mode() const override;
 
  private:
@@ -202,6 +212,11 @@ class AsyncConnection : public Connection {
 
   entity_addr_t _infer_target_addr(const entity_addrvec_t& av);
 
+#ifdef WITH_GLOBAL_CACHE
+  const uint32_t lower_port = 7880;
+  const uint32_t upper_port = 7889;
+#endif
+
   // used only by "read_until"
   uint64_t state_offset;
   Worker *worker;
diff --git a/src/msg/async/ProtocolV2.cc b/src/msg/async/ProtocolV2.cc
index 4b03f5e..d5211fa 100644
--- a/src/msg/async/ProtocolV2.cc
+++ b/src/msg/async/ProtocolV2.cc
@@ -13,6 +13,12 @@
 #include "auth/AuthClient.h"
 #include "auth/AuthServer.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#include <iomanip>
+#endif
+
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
 #define dout_prefix _conn_prefix(_dout)
@@ -360,7 +366,18 @@ CtPtr ProtocolV2::_fault() {
       if (backoff > cct->_conf->ms_max_backoff)
         backoff.set_from_double(cct->_conf->ms_max_backoff);
     }
-
+#ifdef WITH_GLOBAL_CACHE
+    entity_addr_t con_target_addr = connection->get_peer_socket_addr();
+    int con_port = con_target_addr.get_port();
+    uint32_t gc_lower_port = connection->get_port_lower_boundary();
+    uint32_t gc_upper_port = connection->get_port_upper_boundary();
+    if (backoff.sec() >= (__u32)trunc(cct->_conf->ms_max_backoff) && con_port >= gc_lower_port && con_port <= gc_upper_port) {
+      ldout(cct, 1) << __func__ << " reconnect timeout more reset connection con_port " << con_port << dendl;
+      stop();
+      connection->dispatch_queue->queue_reset(connection);
+      return nullptr;
+    }
+#endif
     if (server_cookie) {
       connect_seq++;
     }
@@ -1708,6 +1725,37 @@ CtPtr ProtocolV2::send_auth_request(std::vector<uint32_t> &allowed_methods) {
   vector<uint32_t> preferred_modes;
   auto am = auth_meta;
   connection->lock.unlock();
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  if (plugin->msg_ref->connections.find(connection) != plugin->msg_ref->connections.end()) {
+    ldout(cct, 3) << __func__ << " Client Adaptor: dummy get_auth_request. " << dendl;
+    am->auth_method = CEPH_AUTH_NONE;
+    preferred_modes = { CEPH_CON_MODE_CRC };
+    connection->lock.lock();
+    if (state != AUTH_CONNECTING) {
+      ldout(cct, 1) << __func__ << " state changed!" << dendl;
+      return _fault();
+    }
+  } else {
+    int r = messenger->auth_client->get_auth_request(
+      connection, am.get(),
+      &am->auth_method, &preferred_modes, &bl);
+      connection->lock.lock();
+      if (state != AUTH_CONNECTING) {
+        ldout(cct, 1) << __func__ << " state changed!" << dendl;
+        return _fault();
+      }
+      if (r < 0) {
+        ldout(cct, 0) << __func__ << " get_initial_auth_request returned " << r
+          << dendl;
+        stop();
+        connection->dispatch_queue->queue_reset(connection);
+        return nullptr;
+      }
+  }
+#else
   int r = messenger->auth_client->get_auth_request(
     connection, am.get(),
     &am->auth_method, &preferred_modes, &bl);
@@ -1723,6 +1771,7 @@ CtPtr ProtocolV2::send_auth_request(std::vector<uint32_t> &allowed_methods) {
     connection->dispatch_queue->queue_reset(connection);
     return nullptr;
   }
+#endif
 
   INTERCEPT(9);
 
@@ -1811,6 +1860,37 @@ CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
   ceph_assert(messenger->auth_client);
   auto am = auth_meta;
   connection->lock.unlock();
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  if (plugin->msg_ref->connections.find(connection) != plugin->msg_ref->connections.end()) {
+    ldout(cct, 3) << __func__ << " Client Adaptor: dummy handle_auth_done. " << dendl;
+    connection->lock.lock();
+    if (state != AUTH_CONNECTING) {
+      ldout(cct, 1) << __func__ << " state changed!" << dendl;
+      return _fault();
+    }
+  } else {
+    int r = messenger->auth_client->handle_auth_done(
+      connection,
+      am.get(),
+      auth_done.global_id(),
+      auth_done.con_mode(),
+      auth_done.auth_payload(),
+      &am->session_key,
+      &am->connection_secret);
+
+    connection->lock.lock();
+    if (state != AUTH_CONNECTING) {
+      ldout(cct, 1) << __func__ << " state changed!" << dendl;
+      return _fault();
+    }
+    if (r < 0) {
+      return _fault();
+    }
+  }
+#else
   int r = messenger->auth_client->handle_auth_done(
     connection,
     am.get(),
@@ -1819,6 +1899,7 @@ CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
     auth_done.auth_payload(),
     &am->session_key,
     &am->connection_secret);
+
   connection->lock.lock();
   if (state != AUTH_CONNECTING) {
     ldout(cct, 1) << __func__ << " state changed!" << dendl;
@@ -1827,6 +1908,7 @@ CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
   if (r < 0) {
     return _fault();
   }
+#endif
   auth_meta->con_mode = auth_done.con_mode();
   session_stream_handlers = \
     ceph::crypto::onwire::rxtx_t::create_handler_pair(cct, *auth_meta, false);
diff --git a/src/osdc/Objecter.cc b/src/osdc/Objecter.cc
index bc39114..f753045 100644
--- a/src/osdc/Objecter.cc
+++ b/src/osdc/Objecter.cc
@@ -51,6 +51,18 @@
 #include "common/errno.h"
 #include "common/EventTrace.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#include "client_adaptor/ClientAdaptorPerf.h"
+#include "common/address_helper.h"
+#include <iomanip>
+
+#include <sys/syscall.h>
+#define gettid() syscall(__NR_gettid)
+#endif
+
 using ceph::real_time;
 using ceph::real_clock;
 
@@ -73,7 +85,12 @@ enum {
   l_osdc_op_send_bytes,
   l_osdc_op_resend,
   l_osdc_op_reply,
-
+#ifdef WITH_GLOBAL_CACHE
+  l_osdc_op_gc_active,
+  l_osdc_op_gc_retry,
+  l_osdc_op_gc_hangon,
+  l_osdc_op_gc_resend,
+#endif
   l_osdc_op,
   l_osdc_op_r,
   l_osdc_op_w,
@@ -236,6 +253,30 @@ void Objecter::init()
 {
   ceph_assert(!initialized);
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  int32_t ccm_ret = (static_cast<ClientAdaptorPlugin *>(plugin))->mgr_ref->init_mgr(this);
+  if (ccm_ret){
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Initiate manager failed ret " << ccm_ret << dendl;
+    (static_cast<ClientAdaptorPlugin *>(plugin))->mgr_ref->set_init_flag(false);
+    ceph_abort();
+  }
+  (static_cast<ClientAdaptorPlugin *>(plugin))->mgr_ref->set_init_flag(true);
+  if (cct->_conf.get_val<bool>("global_cache")) {
+    if ((static_cast<ClientAdaptorPlugin *>(plugin))->msg_ref->das_init(this)){
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " Initiate DAS failed, close prefetch" << dendl;
+    }
+  }
+  ldout(cct, 3) << __func__ << "Client Adaptor: PID: " << dec << getpid() << " TID: " << gettid() << dendl;
+  ldout(cct, 3) << __func__ << "Client Adaptor: Objecter pointer: " << hex << this << dendl;
+  if (cct->_conf.get_val<bool>("global_cache_tick")) {
+    plugin->perf_ref->start_record(plugin);
+  }
+  gc_perf = cct->_conf.get_val<bool>("gc_perf");
+#endif
+
   if (!logger) {
     PerfCountersBuilder pcb(cct, "objecter", l_osdc_first, l_osdc_last);
 
@@ -246,6 +287,16 @@ void Objecter::init()
     pcb.add_u64_counter(l_osdc_op_send_bytes, "op_send_bytes", "Sent data", NULL, 0, unit_t(UNIT_BYTES));
     pcb.add_u64_counter(l_osdc_op_resend, "op_resend", "Resent operations");
     pcb.add_u64_counter(l_osdc_op_reply, "op_reply", "Operation reply");
+#ifdef WITH_GLOBAL_CACHE
+    pcb.add_u64_counter(l_osdc_op_gc_active, "op_gc_active",
+            "Operation being sent to gc ");
+    pcb.add_u64_counter(l_osdc_op_gc_resend, "op_gc_resend",
+            "Operation being resent to gc");
+    pcb.add_u64_counter(l_osdc_op_gc_retry, "op_gc_retry",
+            "Operation need to be retried to gc, because of error");
+    pcb.add_u64_counter(l_osdc_op_gc_hangon, "op_gc_hangon",
+            "Operation being hanging because of abnormal PT");
+#endif
 
     pcb.add_u64_counter(l_osdc_op, "op", "Operations");
     pcb.add_u64_counter(l_osdc_op_r, "op_r", "Read operations", "rd",
@@ -400,6 +451,38 @@ void Objecter::shutdown()
 {
   ceph_assert(initialized);
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  if (cct->_conf.get_val<bool>("global_cache_tick")) {
+    plugin->perf_ref->tick_done = true;
+    plugin->perf_ref->threads[0].join();
+    plugin->perf_ref->outfile.close();
+  }
+  if (plugin){
+    plugin->msg_ref->das_remove(this);
+    plugin->mgr_ref->ccm_deregister(this);
+    std::lock_guard l(reg->lock);
+    reg->remove("global_cache", "client_adaptor_plugin");
+  }
+
+  while(!retry_op.op_waiting_for_retry.empty()) {
+    map<uint32_t, std::queue<Op*>>::iterator i = retry_op.op_waiting_for_retry.begin();
+    retry_op.op_waiting_for_retry.erase(i->first);
+  }
+
+  while(!retry_op.reboot_retry_ops.empty()) {
+    map<ceph_tid_t, Op*>::iterator i = retry_op.reboot_retry_ops.begin();
+    retry_op.reboot_retry_ops.erase(i->first);
+  }
+
+  while(!retry_op.hangon_retry_submit_ops.empty()) {
+    retry_op.hangon_retry_submit_ops.pop_back();
+  }
+  ldout(cct, 3) << __func__ << "Client Adaptor: PID: " << dec << getpid() << " TID: " << gettid() << dendl;
+  ldout(cct, 3) << __func__ << "Client Adaptor: Objecter pointer: " << hex << this << dendl;
+#endif
   unique_lock wl(rwlock);
 
   initialized = false;
@@ -1229,17 +1312,41 @@ void Objecter::handle_osd_map(MOSDMap *m)
 	for (map<int,OSDSession*>::iterator p = osd_sessions.begin();
 	     p != osd_sessions.end(); ) {
 	  OSDSession *s = p->second;
-	  _scan_requests(s, skipped_map, cluster_full,
-			 &pool_full_map, need_resend,
-			 need_resend_linger, need_resend_command, sul,
-			 &m->gap_removed_snaps);
+#ifdef WITH_GLOBAL_CACHE
+    if (!(p->first >> 20)) {
+      _scan_requests(s, skipped_map, cluster_full,
+              &pool_full_map, need_resend,
+              need_resend_linger, need_resend_command, sul,
+              &m->gap_removed_snaps);
+    }
+#else
+      _scan_requests(s, skipped_map, cluster_full,
+              &pool_full_map, need_resend,
+              need_resend_linger, need_resend_command, sul,
+              &m->gap_removed_snaps);
+
+#endif
 	  ++p;
+#ifdef WITH_GLOBAL_CACHE
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (plugin->msg_ref->is_node(s->osd)) {
+      // osd means one Global Cache connection
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " bypass close osd session for 0x" << hex << s->osd << dendl;
+    } else if (!osdmap->is_up(s->osd) ||
+	      (s->con &&
+	       s->con->get_peer_addrs() != osdmap->get_addrs(s->osd))) {
+      close_session(s);
+    }
+#else
 	  // osd down or addr change?
 	  if (!osdmap->is_up(s->osd) ||
 	      (s->con &&
 	       s->con->get_peer_addrs() != osdmap->get_addrs(s->osd))) {
 	    close_session(s);
 	  }
+#endif
 	}
 
 	ceph_assert(e == osdmap->get_epoch());
@@ -1772,6 +1879,53 @@ int Objecter::_get_session(int osd, OSDSession **session, shunique_lock& sul)
     return 0;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  map<int,OSDSession*>::iterator p = osd_sessions.find(osd);
+
+  if (p != osd_sessions.end()) {
+    OSDSession *s = p->second;
+    s->get();
+    *session = s;
+    ldout(cct, 20) << __func__ << " s=" << s << " osd=" << osd << " "
+	        << s->get_nref() << dendl;
+    return 0;
+  }
+  if (!sul.owns_lock()) {
+    return -EAGAIN;
+  }
+
+  OSDSession *s = nullptr;
+  // osd_entry = osd;
+  if (plugin->msg_ref->is_node(osd)) {
+    string node_ip = "";
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " osd = 0x" << hex << osd << dendl;
+    uint32_t ret = plugin->msg_ref->get_node_ip(osd, node_ip);
+    if (ret){
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " Get node ip failed, ret " << ret << dendl;
+      ceph_abort();
+    }
+
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " ip address = " << node_ip.c_str() << dendl;
+    entity_addr_t node_addr;
+    entity_addr_from_url(&node_addr, node_ip.c_str());
+    node_addr.set_type(entity_addr_t::TYPE_MSGR2);
+    entity_addrvec_t node_addrs(node_addr);
+    s = new OSDSession(cct, osd);
+    osd_sessions[osd] = s;
+    s->con = messenger->connect_to_osd(node_addrs);
+    plugin->msg_ref->connections.insert((void*)(s->con.get()));
+    for (auto it : plugin->msg_ref->connections) {
+      ldout(cct, 3) << "Client Adaptor: con = " << it << dendl;
+    }
+  } else {
+    s = new OSDSession(cct, osd);
+    osd_sessions[osd] = s;
+    s->con = messenger->connect_to_osd(osdmap->get_addrs(osd));
+  }
+#else
   map<int,OSDSession*>::iterator p = osd_sessions.find(osd);
   if (p != osd_sessions.end()) {
     OSDSession *s = p->second;
@@ -1787,6 +1941,7 @@ int Objecter::_get_session(int osd, OSDSession **session, shunique_lock& sul)
   OSDSession *s = new OSDSession(cct, osd);
   osd_sessions[osd] = s;
   s->con = messenger->connect_to_osd(osdmap->get_addrs(osd));
+#endif
   s->con->set_priv(RefCountedPtr{s});
   logger->inc(l_osdc_osd_session_open);
   logger->set(l_osdc_osd_sessions, osd_sessions.size());
@@ -2181,6 +2336,12 @@ void Objecter::tick()
     }
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  RetryOp::unique_lock retry_op_rl(retry_op.retrylock);
+  ldout(cct, 2) << " tick hangon_retry_submit_ops size " <<retry_op.hangon_retry_submit_ops.size()<< dendl;
+  ldout(cct, 2) << " tick op_waiting_for_retry size " <<retry_op.op_waiting_for_retry.size()<< dendl;
+  retry_op_rl.unlock();
+#endif
   // Make sure we don't reschedule if we wake up after shutdown
   if (initialized) {
     tick_event = timer.reschedule_me(ceph::make_timespan(
@@ -2296,6 +2457,16 @@ void Objecter::_send_op_account(Op *op)
   }
 
   logger->inc(l_osdc_op_active);
+#ifdef WITH_GLOBAL_CACHE
+  if (gc_perf) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (plugin->msg_ref->filter_msg(&op->target)) {
+      logger->inc(l_osdc_op_gc_active);
+    }
+  }
+#endif
   logger->inc(l_osdc_op);
 
   if ((op->target.flags & (CEPH_OSD_FLAG_READ | CEPH_OSD_FLAG_WRITE)) ==
@@ -2363,10 +2534,25 @@ void Objecter::_op_submit(Op *op, shunique_lock& sul, ceph_tid_t *ptid)
   // pick target
   ceph_assert(op->session == NULL);
   OSDSession *s = NULL;
+#ifdef WITH_GLOBAL_CACHE
+  bool pt_stat = true;
+  bool check_for_latest_map = _calc_pt_target(&op->target, nullptr, pt_stat)
+    == RECALC_OP_TARGET_POOL_DNE;
 
+  if (!pt_stat) {
+    unique_lock rl(retry_op.retrylock);
+    retry_op.hangon_retry_submit_ops.push_back(op);
+    rl.unlock();
+    if (gc_perf) {
+        logger->inc(l_osdc_op_gc_hangon);
+    }
+    ldout(cct, 1) << " this " << this << " " << __func__ << " op " << op << "pt unnormal, hang on IO waiting for retry by pt normal trigger " << dendl;
+    return;
+  }
+#else
   bool check_for_latest_map = _calc_target(&op->target, nullptr)
     == RECALC_OP_TARGET_POOL_DNE;
-
+#endif
   // Try to get a session, including a retry if we need to take write lock
   int r = _get_session(op->target.osd, &s, sul);
   if (r == -EAGAIN ||
@@ -2382,8 +2568,24 @@ void Objecter::_op_submit(Op *op, shunique_lock& sul, ceph_tid_t *ptid)
       // map changed; recalculate mapping
       ldout(cct, 10) << __func__ << " relock raced with osdmap, recalc target"
 		     << dendl;
+#ifdef WITH_GLOBAL_CACHE
+      check_for_latest_map = _calc_pt_target(&op->target, nullptr, pt_stat)
+        == RECALC_OP_TARGET_POOL_DNE;
+
+      if (!pt_stat) {
+        unique_lock rl(retry_op.retrylock);
+        retry_op.hangon_retry_submit_ops.push_back(op);
+        rl.unlock();
+        if (gc_perf) {
+            logger->inc(l_osdc_op_gc_hangon);
+        }
+        ldout(cct, 1) << __func__ << " op " << op << "pt unnormal, hang on IO waiting for retry by pt normal trigger " << dendl;
+        return;
+      }
+#else
       check_for_latest_map = _calc_target(&op->target, nullptr)
-	== RECALC_OP_TARGET_POOL_DNE;
+	      == RECALC_OP_TARGET_POOL_DNE;
+#endif
       if (s) {
 	put_session(s);
 	s = NULL;
@@ -2453,6 +2655,17 @@ void Objecter::_op_submit(Op *op, shunique_lock& sul, ceph_tid_t *ptid)
   _session_op_assign(s, op);
 
   if (need_send) {
+#ifdef WITH_GLOBAL_CACHE
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (plugin->msg_ref->filter_msg_by_op(op)){
+      plugin->msg_ref->das_update_info(this, op);
+      if (cct->_conf.get_val<bool>("global_cache_tick")) {
+        plugin->perf_ref->start_tick(op);
+      }
+    }
+#endif
     _send_op(op);
   }
 
@@ -2770,6 +2983,231 @@ void Objecter::_prune_snapc(
   }
 }
 
+#ifdef WITH_GLOBAL_CACHE
+int Objecter::_calc_pt_target(op_target_t *t, Connection *con, bool &pt_status, bool any_change)
+{
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  if (plugin->msg_ref->filter_msg(t)){
+    t->target_oid = t->base_oid;
+    t->target_oloc = t->base_oloc;
+    ldout(cct, 3) << " Client Adaptor: " << __func__ << " msg filter pass to Global Cache" << dendl;
+    uint64_t pool_id = t->base_oloc.pool;
+    uint32_t pt_id = 0;
+    int32_t node_id = plugin->msg_ref->get_node_id(t->base_oid.name, pool_id, pt_id);
+    if (node_id < 0) {
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " Get node id failed ret " << node_id << dendl;
+      ceph_abort();
+    }
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Send to PT " << pt_id << dendl;
+    t->actual_pgid.pgid.set_pool(t->base_oloc.pool);
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Pool ID = " << t->actual_pgid.pgid.m_pool << " pt version =" << (pool_id >> 32)<< dendl;
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Seed = " << t->actual_pgid.pgid.m_seed << dendl;
+    t->actual_pgid.pgid.set_ps(pt_id);
+    t->osd = node_id;
+    pt_status = plugin->mgr_ref->get_pt_status(pt_id);
+    return RECALC_OP_TARGET_NO_ACTION;
+  } else {
+    // rwlock is locked
+    bool is_read = t->flags & CEPH_OSD_FLAG_READ;
+    bool is_write = t->flags & CEPH_OSD_FLAG_WRITE;
+    t->epoch = osdmap->get_epoch();
+    ldout(cct,20) << __func__ << " epoch " << t->epoch
+      << " base " << t->base_oid << " " << t->base_oloc
+      << " precalc_pgid " << (int)t->precalc_pgid
+      << " pgid " << t->base_pgid
+      << (is_read ? " is_read" : "")
+      << (is_write ? " is_write" : "")
+      << dendl;
+
+    const pg_pool_t *pi = osdmap->get_pg_pool(t->base_oloc.pool);
+    if (!pi) {
+      t->osd = -1;
+      return RECALC_OP_TARGET_POOL_DNE;
+    }
+    ldout(cct,30) << __func__ << " base pi " << pi
+      << " pg_num " << pi->get_pg_num() << dendl;
+
+    bool force_resend = false;
+    if (osdmap->get_epoch() == pi->last_force_op_resend) {
+      if (t->last_force_resend < pi->last_force_op_resend) {
+        t->last_force_resend = pi->last_force_op_resend;
+	force_resend = true;
+      } else if (t->last_force_resend == 0) {
+        force_resend = true;
+      }
+    }
+
+    //apply tiering
+    t->target_oid = t->base_oid;
+    t->target_oloc = t->base_oloc;
+    if ((t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {
+      if (is_read && pi->has_read_tier())
+	t->target_oloc.pool = pi->read_tier;
+      if (is_write && pi->has_write_tier())
+	t->target_oloc.pool = pi->write_tier;
+      pi = osdmap->get_pg_pool(t->target_oloc.pool);
+      if (!pi) {
+        t->osd = -1;
+	return RECALC_OP_TARGET_POOL_DNE;
+      }
+    }
+
+    pg_t pgid;
+    if (t->precalc_pgid) {
+      ceph_assert(t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY);
+      ceph_assert(t->base_oid.name.empty()); //make sure this is a pg op 
+      ceph_assert(t->base_oloc.pool == (int64_t)t->base_pgid.pool());
+      pgid = t->base_pgid;
+    } else {
+      int ret = osdmap->object_locator_to_pg(t->target_oid, t->target_oloc,
+	      pgid);
+      if (ret == -ENOENT) {
+        t->osd = -1;
+	return RECALC_OP_TARGET_POOL_DNE;
+      }
+    }
+    ldout(cct,20) << __func__ << " target " << t->target_oid << " "
+      << t->target_oloc << " -> pgid " << pgid << dendl;
+    ldout(cct,30) << __func__ << " target pi " << pi
+      <<" pg_num " << pi->get_pg_num() << dendl;
+    t->pool_ever_existed = true;
+
+    int size = pi->size;
+    int min_size = pi->min_size;
+    unsigned pg_num = pi->get_pg_num();
+    unsigned pg_num_pending = pi->get_pg_num_pending();
+    int up_primary, acting_primary;
+    vector<int> up, acting;
+    osdmap->pg_to_up_acting_osds(pgid, &up, &up_primary,
+	      &acting, &acting_primary);
+    bool sort_bitwise = osdmap->test_flag(CEPH_OSDMAP_SORTBITWISE);
+    bool recovery_deletes = osdmap->test_flag(CEPH_OSDMAP_RECOVERY_DELETES);
+    unsigned prev_seed = ceph_stable_mod(pgid.ps(), t->pg_num, t->pg_num_mask);
+    pg_t prev_pgid(prev_seed, pgid.pool());
+    if (any_change && PastIntervals::is_new_interval(
+    t->acting_primary,
+    acting_primary,
+    t->acting,
+    acting,
+    t->up_primary,
+    up_primary,
+    t->up,
+    up,
+    t->size,
+    size,
+    t->min_size,
+    min_size,
+    t->pg_num,
+    pg_num,
+    t->pg_num_pending,
+    pg_num_pending,
+    t->sort_bitwise,
+    sort_bitwise,
+    t->recovery_deletes,
+    recovery_deletes,
+    prev_pgid)) {
+      force_resend = true;
+    }
+
+    bool unpaused = false;
+    bool should_be_paused = target_should_be_paused(t);
+    if (t->paused && !should_be_paused) {
+      unpaused = true;
+    }
+    t->paused = should_be_paused;
+
+    bool legacy_change = 
+      t->pgid != pgid ||
+        is_pg_changed(
+    t->acting_primary, t->acting, acting_primary, acting,
+    t->used_replica || any_change);
+    bool split_or_merge = false;
+    if (t->pg_num) {
+      split_or_merge = 
+	prev_pgid.is_split(t->pg_num, pg_num, nullptr) ||
+	prev_pgid.is_merge_source(t->pg_num, pg_num, nullptr) ||
+	prev_pgid.is_merge_target(t->pg_num, pg_num);
+    }
+
+    if (legacy_change || split_or_merge || force_resend) {
+      t->pgid = pgid;
+      t->acting = acting;
+      t->acting_primary = acting_primary;
+      t->up_primary = up_primary;
+      t->up = up;
+      t->size = size;
+      t->min_size = min_size;
+      t->pg_num = pg_num;
+      t->pg_num_mask = pi->get_pg_num_mask();
+      t->pg_num_pending = pg_num_pending;
+      osdmap->get_primary_shard(
+	pg_t(ceph_stable_mod(pgid.ps(), t->pg_num, t->pg_num_mask), pgid.pool()),
+	&t->actual_pgid);
+      t->sort_bitwise = sort_bitwise;
+      t->recovery_deletes = recovery_deletes;
+      ldout(cct, 10) << __func__ << " "
+	<< " raw pgid " << pgid << " -> actual " << t->actual_pgid
+        << " acting " << acting
+        << " primary " << acting_primary << dendl;
+      t->used_replica = false;
+      if (acting_primary == -1) {
+        t->osd = -1;
+      } else {
+        int osd;
+	bool read = is_read && !is_write;
+	if (read && (t->flags & CEPH_OSD_FLAG_BALANCE_READS)) {
+    int p = rand() % acting.size();
+    if (p)
+      t->used_replica = true;
+    osd = acting[p];
+    ldout(cct, 10) << " chose random osd." << osd << " of " << acting
+	    << dendl;
+	} else if (read && (t->flags & CEPH_OSD_FLAG_LOCALIZE_READS) &&
+      acting.size() > 1) {
+    
+
+    int best = -1;
+    int best_locality = 0;
+    for (unsigned i = 0; i < acting.size(); ++i) {
+      int locality = osdmap->crush->get_common_ancestor_distance(
+      cct, acting[i], crush_location);
+      ldout(cct, 20) << __func__ << " localize: rank " << i
+	<< " osd." << acting[i]
+	<< " locality " << locality << dendl;
+      if (i == 0 ||
+          (locality >= 0 && best_locality >= 0 &&
+	   locality < best_locality) ||
+	   (best_locality < 0 && locality >= 0)) {
+        best = i;
+        best_locality = locality;
+        if (i)
+          t->used_replica = true;
+      }
+    }
+    ceph_assert(best >= 0);
+    osd = acting[best];
+	} else {
+    osd = acting_primary;
+	}
+	t->osd = osd;
+      }      
+    }
+    if (legacy_change || unpaused || force_resend) {
+      return RECALC_OP_TARGET_NEED_RESEND;
+    }
+    if (split_or_merge &&
+	(osdmap->require_osd_release >= CEPH_RELEASE_LUMINOUS ||
+	 HAVE_FEATURE(osdmap->get_xinfo(acting_primary).features,
+	   RESEND_ON_SPLIT))) {
+      return RECALC_OP_TARGET_NEED_RESEND;
+    }
+    return RECALC_OP_TARGET_NO_ACTION;
+  }
+}
+#endif
+
 int Objecter::_calc_target(op_target_t *t, Connection *con, bool any_change)
 {
   // rwlock is locked
@@ -2969,6 +3407,7 @@ int Objecter::_calc_target(op_target_t *t, Connection *con, bool any_change)
   return RECALC_OP_TARGET_NO_ACTION;
 }
 
+
 int Objecter::_map_session(op_target_t *target, OSDSession **s,
 			   shunique_lock& sul)
 {
@@ -3139,7 +3578,16 @@ void Objecter::_finish_op(Op *op, int r)
   }
 
   logger->dec(l_osdc_op_active);
-
+#ifdef WITH_GLOBAL_CACHE
+  if (gc_perf) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (plugin->msg_ref->filter_msg(&op->target)) {
+      logger->dec(l_osdc_op_gc_active);
+    }
+  }
+#endif
   ceph_assert(check_latest_map_ops.find(op->tid) == check_latest_map_ops.end());
 
   inflight_ops--;
@@ -3271,6 +3719,38 @@ void Objecter::_send_op(Op *op)
   if (op->trace.valid()) {
     m->trace.init("op msg", nullptr, &op->trace);
   }
+
+#ifdef WITH_GLOBAL_CACHE
+  ldout(cct, 3) << "Client Adaptor: " << __func__ << " msg-type = 0x" << hex << m->get_type() << dendl;
+  if (m->get_type() == CEPH_MSG_OSD_OP) {
+    MOSDOp *mosdop = static_cast<MOSDOp *>(m);
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " MOSDOp object name = " << mosdop->get_oid().name << dendl;
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Request ID = " << mosdop->get_reqid().tid << dendl;
+    string cname, mname;
+    uint32_t pt_index = mosdop->get_pg().m_seed;
+    uint64_t pool_id = mosdop->get_pg().m_pool;
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " PT ID = " << pt_index << " Pool ID = " << pool_id << dendl;
+    int index = 0;
+    for (auto op : mosdop->ops) {
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " index = " << index
+	      << " op-code = 0x" << hex << op.op.op << dendl;
+
+      index++;
+      if (op.op.op == CEPH_OSD_OP_READ || op.op.op == CEPH_OSD_OP_WRITE || op.op.op == CEPH_OSD_OP_SPARSE_READ ||
+	     op.op.op == CEPH_OSD_OP_WRITEFULL || op.op.op == CEPH_OSD_OP_SYNC_READ) {
+        ldout(cct, 3) << "Client Adaptor: " << __func__ << " offset = 0x" << hex << op.op.extent.offset
+               << " length = 0x" << hex << op.op.extent.length << dendl;
+      }
+      if (op.op.op == CEPH_OSD_OP_CALL) {
+        auto bp = op.indata.cbegin();
+	      bp.copy(op.op.cls.class_len, cname);
+	      bp.copy(op.op.cls.method_len, mname);
+	      ldout(cct, 3) << "Client Adaptor: " << __func__ << " op call class: " << cname << " method: " << mname << dendl;
+      }
+    }
+  }
+#endif
+
   op->session->con->send_message(m);
 }
 
@@ -3326,6 +3806,82 @@ int Objecter::take_linger_budget(LingerOp *info)
   return 1;
 }
 
+#ifdef WITH_GLOBAL_CACHE
+void Objecter::retry_op_submit(vector<uint32_t> ready_pt_id)
+{
+  if (!ready_pt_id.size()) {
+    ldout(cct, 3) << __func__ << "ccm pt change notify, normal pt size 0 " << dendl;
+    return;
+  }
+  std::queue<std::queue<Op*>> deal_op;
+  map<uint32_t, std::queue<Op*>>::iterator iter;
+  unique_lock rl(retry_op.retrylock);
+  for (uint32_t i = 0; i < ready_pt_id.size(); i++) {
+    iter = retry_op.op_waiting_for_retry.find(ready_pt_id[i]);
+    if (iter != retry_op.op_waiting_for_retry.end()) {
+      deal_op.push(iter->second);
+      retry_op.op_waiting_for_retry.erase(iter);
+    }
+  }
+  rl.unlock();
+  ldout(cct, 3) << __func__ << "resend op queue size " << deal_op.size() << " remain resend size " << retry_op.op_waiting_for_retry.size() << dendl;
+
+  uint32_t resend_errort_op_num = 0;
+  while(!deal_op.empty()) {
+    std::queue<Op*> op_queue = deal_op.front();
+    deal_op.pop();
+    while(!op_queue.empty()) {
+      Op* op = op_queue.front();
+      op_queue.pop();
+      if (gc_perf) {
+        logger->dec(l_osdc_op_gc_retry);
+      }
+      shunique_lock sul(rwlock, ceph::acquire_shared);
+      _op_submit(op, sul, NULL);
+      resend_errort_op_num++;
+    }
+  }
+  ldout(cct, 3) << __func__ << "resend errort op number " << resend_errort_op_num << dendl;
+
+
+  // reboot inflight ops
+
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+
+  rl.lock();
+  
+  std::queue<Op*> op_queue;
+  // hang on io retry
+  uint32_t resend_hangon_op_num = 0;
+  for (auto iter = retry_op.hangon_retry_submit_ops.begin(); iter != retry_op.hangon_retry_submit_ops.end(); ) {
+    Op *op = static_cast<Op*>(*iter);
+    uint32_t pt_id = op->target.actual_pgid.pgid.m_seed;
+    if (plugin->mgr_ref->get_pt_status(pt_id)) {
+      op_queue.push(op);
+      resend_hangon_op_num++;
+      retry_op.hangon_retry_submit_ops.erase(iter);
+    } else {
+      ldout(cct, 1) << __func__ << " resend hangon_retry_submit_ops pt " << pt_id << " unnormal, resend failed"<< dendl;
+      ++iter;
+    }
+  }
+  ldout(cct, 3) << __func__ << " should resend hangon op number " << resend_hangon_op_num << " left hangon size " << retry_op.hangon_retry_submit_ops.size() << dendl;
+  rl.unlock();
+
+  while (!op_queue.empty()) {
+    Op* op = op_queue.front();
+    op_queue.pop();
+    if (gc_perf) {
+        logger->dec(l_osdc_op_gc_hangon);
+    }
+    shunique_lock sul(rwlock, ceph::acquire_shared);
+    _op_submit(op, sul, NULL);
+  }
+}
+#endif
+
 /* This function DOES put the passed message before returning */
 void Objecter::handle_osd_op_reply(MOSDOpReply *m)
 {
@@ -3348,7 +3904,11 @@ void Objecter::handle_osd_op_reply(MOSDOpReply *m)
     m->put();
     return;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+#endif
   OSDSession::unique_lock sl(s->lock);
 
   map<ceph_tid_t, Op *>::iterator iter = s->ops.find(tid);
@@ -3406,7 +3966,6 @@ void Objecter::handle_osd_op_reply(MOSDOpReply *m)
   Context *onfinish = 0;
 
   int rc = m->get_result();
-
   if (m->is_redirect_reply()) {
     ldout(cct, 5) << " got redirect reply; redirecting" << dendl;
     if (op->onfinish)
@@ -3443,8 +4002,48 @@ void Objecter::handle_osd_op_reply(MOSDOpReply *m)
     return;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  if (errort_filter(rc) && m->get_oid().name.find("rbd_data") != string::npos) {
+       ldout(cct,1) << " op " << op << " error code " << rc << ", client resubmitting" << dendl;
+       if (op->onfinish)
+         num_in_flight--;
+       _session_op_remove(s, op);
+       sl.unlock();
+
+       op->tid = 0;
+       op->target.flags &= ~(CEPH_OSD_FLAG_BALANCE_READS |
+                         CEPH_OSD_FLAG_LOCALIZE_READS);
+       op->target.pgid = pg_t();
+       unique_lock rl(retry_op.retrylock);
+       std::queue<Op*> insert_op;
+       uint32_t pt_id = m->get_pg().m_seed;
+       map<uint32_t, std::queue<Op*>>::iterator iter = retry_op.op_waiting_for_retry.find(pt_id);
+       if (iter != retry_op.op_waiting_for_retry.end()) {
+               iter->second.push(op);
+       } else {
+               insert_op.push(op);
+               retry_op.op_waiting_for_retry[pt_id] = insert_op;
+       }
+       rl.unlock();
+       if (gc_perf) {
+                logger->inc(l_osdc_op_gc_retry);
+       }
+       m->put();
+       return;
+  }
+#endif
   sul.unlock();
 
+#ifdef WITH_GLOBAL_CACHE
+  if (cct->_conf.get_val<bool>("global_cache_tick")) {
+    if (plugin->msg_ref->filter_msg_by_op(op)){
+      plugin->perf_ref->end_tick(op);
+      plugin->perf_ref->record_op(op);
+      plugin->perf_ref->total_in_flight += num_in_flight;
+    }
+  }
+#endif
+
   if (op->objver)
     *op->objver = m->get_user_version();
   if (op->reply_epoch)
@@ -4399,6 +4998,53 @@ bool Objecter::ms_handle_reset(Connection *con)
     if (session) {
       ldout(cct, 1) << "ms_handle_reset " << con << " session " << session
 		    << " osd." << session->osd << dendl;
+#ifdef WITH_GLOBAL_CACHE
+  if (session->osd >> 20) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    OSDSession::unique_lock sl(session->lock);
+    if (session->con) {
+        plugin->msg_ref->connections.erase((void *)(session->con.get()));
+        session->con->set_priv(NULL);
+        session->con->mark_down();
+    }
+    map<ceph_tid_t,Op*> resend;
+    for (map<ceph_tid_t,Op*>::iterator p = session->ops.begin(); p != session->ops.end(); ++p) {
+        Op *op = p->second;
+        resend[op->tid] = op;
+    }
+
+    ldout(cct, 3) << __func__ << " resend size " << resend.size()  << dendl;
+    for (map<ceph_tid_t,Op*>::iterator p = resend.begin(); p!= resend.end(); ++p ) {
+      Op *op = p->second;
+      if (op->onfinish) {
+        num_in_flight--;
+      }
+      _session_op_remove(session, op);
+    }
+    sl.unlock();
+    close_session(session);
+
+    if (!(initialized && osdmap->is_up(session->osd))) {
+      ldout(cct, 1) << "ms_handle_reset aborted,initialized=" << initialized << dendl;
+      wl.unlock();
+
+      for (map<ceph_tid_t,Op*>::iterator p = resend.begin(); p!= resend.end(); ++p ) {
+        Op *op = p->second;
+        shunique_lock sul(rwlock, ceph::acquire_shared);
+        ldout(cct, 3) << __func__ << " resend submit op " << op  << dendl;
+        if (gc_perf && plugin->msg_ref->filter_msg_by_op(op)) {
+          logger->dec(l_osdc_op_gc_active);
+          logger->inc(l_osdc_op_gc_resend);
+        }
+        _op_submit(op, sul, NULL);
+      }
+
+      return false;
+    }
+  }
+#endif
       // the session maybe had been closed if new osdmap just handled
       // says the osd down
       if (!(initialized && osdmap->is_up(session->osd))) {
diff --git a/src/osdc/Objecter.h b/src/osdc/Objecter.h
index ca8d85f..9723662 100644
--- a/src/osdc/Objecter.h
+++ b/src/osdc/Objecter.h
@@ -1341,7 +1341,13 @@ public:
     int incarnation;
 
     op_target_t target;
-
+#ifdef WITH_GLOBAL_CACHE
+    struct perf_tick_t {
+      struct timeval start;
+      struct timeval end;
+    };
+    perf_tick_t perf_tick;
+#endif
     ConnectionRef con;  // for rx buffer only
     uint64_t features;  // explicitly specified op features
 
@@ -1863,9 +1869,29 @@ public:
 
   bool osdmap_full_flag() const;
   bool osdmap_pool_full(const int64_t pool_id) const;
+#ifdef WITH_GLOBAL_CACHE
+
+
+  struct RetryOp {
+    map<uint32_t, std::queue<Op*>>op_waiting_for_retry; //error code retry
+    map<ceph_tid_t, Op*> reboot_retry_ops; //reboot retry ops
+    std::vector<Op*> hangon_retry_submit_ops; //reboot hang on ops
+    mutable std::shared_mutex retrylock;
+    using unique_lock = std::unique_lock<decltype(retrylock)>;
+  };
+
+  RetryOp retry_op;
 
+  void retry_op_submit(vector<uint32_t> ready_pt_id);
+  bool gc_perf;
+#endif
  private:
 
+#ifdef WITH_GLOBAL_CACHE
+  int _calc_pt_target(op_target_t *t, Connection *con,
+            bool &pt_status, bool any_change = false);
+
+#endif
   /**
    * Test pg_pool_t::FLAG_FULL on a pool
    *
@@ -2057,6 +2083,34 @@ private:
     return std::forward<Callback>(cb)(*osdmap, std::forward<Args>(args)...);
   }
 
+  bool errort_filter(errorcode32_t returnCode)
+  {
+    switch (returnCode) {
+      case -EINTR:
+      case -EBUSY:
+      case -ETXTBSY:
+      case -ENOSPC:
+      case -EDEADLK:
+      case -EWOULDBLOCK:
+      case -ETIME:
+      case -ECOMM:
+      case -ERESTART:
+      case -ENETDOWN:
+      case -ENETRESET:
+      case -ECONNRESET:
+      case -ENOBUFS:
+      case -ETIMEDOUT:
+      case -EHOSTDOWN:
+      case -EALREADY:
+      case -ENOMEDIUM:
+      case -ECANCELED:
+        return true;
+      default:
+        return false;
+    }
+    return false;
+  }
+
 
   /**
    * Tell the objecter to throttle outgoing ops according to its
diff --git a/src/test/CMakeLists.txt b/src/test/CMakeLists.txt
index 5dcee16..fc342cc 100644
--- a/src/test/CMakeLists.txt
+++ b/src/test/CMakeLists.txt
@@ -31,6 +31,13 @@ add_subdirectory(fs)
 add_subdirectory(journal)
 add_subdirectory(libcephfs)
 add_subdirectory(librados)
+
+# Client adaptor
+if(WITH_GLOBAL_CACHE)
+	add_subdirectory(ClientAdaptorTest)
+	add_subdirectory(ServerAdaptorSimulate)
+endif()
+
 add_subdirectory(librados_test_stub)
 if(WITH_LIBRADOSSTRIPER)
   add_subdirectory(libradosstriper)
diff --git a/src/test/ClientAdaptorTest/CMakeLists.txt b/src/test/ClientAdaptorTest/CMakeLists.txt
new file mode 100644
index 0000000..2eb494c
--- /dev/null
+++ b/src/test/ClientAdaptorTest/CMakeLists.txt
@@ -0,0 +1,11 @@
+# unittest_client_adaptor
+
+add_executable(client_adaptor_plugin_test
+  ClientAdaptorTest.cc
+  $<TARGET_OBJECTS:unit-main>
+  )
+target_link_libraries(client_adaptor_plugin_test global ${UNITTEST_LIBS} ceph_client_adaptor_plugin)
+
+# add_ceph_unittest(client_adaptor_test)
+
+message(STATUS "Client adaptor test cmake executing...")
diff --git a/src/test/ClientAdaptorTest/ClientAdaptorTest.cc b/src/test/ClientAdaptorTest/ClientAdaptorTest.cc
new file mode 100644
index 0000000..2d69eb1
--- /dev/null
+++ b/src/test/ClientAdaptorTest/ClientAdaptorTest.cc
@@ -0,0 +1,389 @@
+;/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include <string.h>
+#include <iomanip>
+
+#include "gtest/gtest.h"
+#include "global/global_context.h"
+
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#include "client_adaptor/ClientAdaptorPerf.h"
+#include "client_adaptor/open_ccm.h"
+#include "osdc/Objecter.h"
+
+class ClientAdaptorCcmMock : public ClientAdaptorMgr{
+public:
+  ClientAdaptorCcmMock(){}
+  ~ClientAdaptorCcmMock() override {}
+  int32_t init_mgr(Objecter *obj) override {
+    std::cout << "Client Adaptor: Init CCM Mock successfully" << std::endl;
+    return 0;
+  }
+
+  int32_t get_pt_num(uint32_t& num) override {
+    num = 32;
+    std::cout << "Client Adaptor: Get total PT number is" << num << std::endl;
+
+    return 0;
+  }
+
+  int32_t get_pt_entry(uint32_t pt_index, PTViewPtEntry* entry) override {
+    std::cout << "Client Adaptor: PT index is" << pt_index << std::endl;
+    entry->masterNode = pt_index % 3;
+    std::cout << "Client Adaptor: PT entry master node id is" << entry->masterNode << std::endl;
+    return 0;
+  }
+
+  int32_t get_node_info(uint32_t node_id, NodeInfo* node_info) override {
+    strcpy(node_info->publicAddrStr, "172.0.0.1");
+    node_info->ports[0] = 68;
+    node_info->portNum = 1;
+    return 0 ;
+  }
+
+  int32_t add_snap_to_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id) {
+    return 0;
+  }
+
+  int32_t remove_snap_from_gc(const int64_t pool_id, const std::string &image_id, const uint64_t snap_id) {
+    return 0;
+  }
+
+  int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id) {
+    return 0;
+  }
+
+  const string name() override {
+      return "ClientAdaptorCcmMock";
+  }
+  bool get_pt_status(uint32_t pt_id) {
+    return true;
+  }
+  void ccm_deregister(Objecter *obj) {}
+};
+
+class ClientAdaptorTest : public ::testing::Test,
+	public ::testing::WithParamInterface<const char*> {
+public:
+  string plugin;
+
+  ClientAdaptorTest(){
+  }
+  ~ClientAdaptorTest() override {
+  }
+
+  void SetUp() override {
+    std::cout << "Client Adaptor: test setup" << std::endl;
+
+    return;
+  }
+  void TearDown() override {
+    std::cout << "Client Adaptor: test teardown" << std::endl;
+    PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+    lock_guard l(reg->lock);
+    reg->remove("global_cache", "client_adaptor_plugin");
+
+    return;
+  }
+};
+
+TEST_P(ClientAdaptorTest, PluginTest)
+{
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+
+  auto dirName = g_ceph_context->_conf.get_val<string>("plugin_dir");
+  std::cout << "Client Adaptor: plugin diractory name = " << dirName << std::endl;
+
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  std::cout << "Plugin address = " << plugin << std::endl;
+  
+
+  std::cout << "Client Adaptor: plugin name = " << plugin->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorPlugin", plugin->name().c_str());
+
+  auto msg=plugin->msg_ref;
+  std::cout << "Client Adaptor: msg name = " << msg->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorMsg", msg->name().c_str());
+
+  auto mgr=plugin->mgr_ref;
+  std::cout << "Client Adaptor: mgr name = " << mgr->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorCcm", mgr->name().c_str());
+
+  auto perf=plugin->perf_ref;
+  std::cout << "Client Adaptor: perf name = " << mgr->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorPerf", perf->name().c_str());
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+
+  return;
+}
+
+TEST_P(ClientAdaptorTest, PluginRegistryTest)
+{
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+
+  std::cout << "Client Adaptor: Plugin registry map before global cache insert:" << std::endl;
+  for(auto it : reg->plugins) {
+  std::cout << it.first << "---" << it.second << std::endl;
+  }
+
+  std::cout << "Client Adaptor: Plugin registry map after global cache insert:" << std::endl;
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  std::cout << "Client Adaptor: Plugin address = " << plugin << std::endl;
+  
+  for(auto it : reg->plugins) {
+  std::cout << "Client Adaptor: " << it.first << "---" << it.second << std::endl;
+  }
+
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  return;
+}
+
+TEST_P(ClientAdaptorTest, CalNodeIpNormalTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  
+  ClientAdaptorCcmMock* ccm_mock = new ClientAdaptorCcmMock();
+  ClientAdaptorMgr* mgr = plugin->msg_ref->get_mgr();
+  std::cout << "Client Adaptor: Before DI Mgr subclass is " << mgr->name() << std::endl;
+  plugin->msg_ref->set_mgr(ccm_mock);
+  mgr = plugin->msg_ref->get_mgr();
+  std::cout << "Client Adaptor: After DI Mgr subclass is " << mgr->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorCcmMock", mgr->name().c_str());
+
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  uint64_t pool_id = 3;
+  uint32_t pt_index;
+  uint32_t node_id = plugin->msg_ref->get_node_id(obj_name, pool_id, pt_index);
+  std::cout << "Client Adaptor: PT id  is " << pt_index << std::endl;
+  std::cout << "Client Adaptor: Hashed node id is 0x " << hex << node_id << std::endl;
+  EXPECT_EQ((uint32_t)0x100010, node_id);
+
+  string node_ip = "";
+  EXPECT_EQ(0, plugin->msg_ref->get_node_ip(node_id, node_ip));
+  std::cout << "Client Adaptor: Node IP =  " << node_ip << std::endl;
+  EXPECT_STREQ("tcp://172.0.0.1:68", node_ip.c_str());
+
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm_mock;
+  return;
+}
+
+TEST_P(ClientAdaptorTest, DasUpdateInfoTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  
+  Objecter tObj(g_ceph_context, NULL, NULL, NULL, 0, 0);
+  plugin->msg_ref->das_init(&tObj);
+  const char *cls = "hello";
+  const char *method = "say_hello";
+  string obj_name = "70ac98a7:::7a3ead19-df4f-4019-a4c2-38c52299e348.4389.1_test5";
+  bufferlist indata;
+  vector<OSDOp> nops(1);
+  OSDOp &op = nops[0];
+
+  op.op.op = CEPH_OSD_OP_CALL;
+  op.op.cls.class_len = strlen(cls);
+  op.op.cls.method_len = strlen(method);
+  op.op.cls.indata_len = indata.length();
+  op.indata.append(cls, op.op.cls.class_len);
+  op.indata.append(method, op.op.cls.method_len);
+  op.indata.append(indata);
+  Objecter::Op *objecter_op =
+	  new Objecter::Op(object_t(obj_name), object_locator_t(), nops, CEPH_OSD_FLAG_EXEC, NULL, NULL ,NULL, nullptr);
+
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  objecter_op->target.flags = CEPH_OSD_FLAG_WRITE;
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  objecter_op->target.flags = CEPH_OSD_FLAG_READ;
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  objecter_op->target.base_oid.name = "rbd_data-135421846e0f-0000000000000056";
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  objecter_op->target.base_oid.name = "rbd_data.135421846e0f.0000000000000056";
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  OSDOp rop;
+  rop.op.op = CEPH_OSD_OP_READ;
+  rop.op.extent.offset = 10;
+  rop.op.extent.length = 4096;
+  objecter_op->ops.push_back(rop);
+  plugin->msg_ref->das_update_info(&tObj, objecter_op);
+  objecter_op->put();
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  return;
+}
+
+extern void das_req_prefetch(DasKvParam *params);
+TEST_P(ClientAdaptorTest, DasPushTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  ClientAdaptorCcmMock* ccm_mock = new ClientAdaptorCcmMock();
+  ClientAdaptorMgr* mgr = plugin->msg_ref->get_mgr();
+
+  std::cout << "Client Adaptor: Before DI Mgr subclass is " << mgr->name() << std::endl;
+  plugin->msg_ref->set_mgr(ccm_mock);
+  mgr = plugin->msg_ref->get_mgr();
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  Objecter tObj(g_ceph_context, NULL, NULL, NULL, 0, 0);
+  DasKvParam *params = reinterpret_cast<DasKvParam*>(new char[sizeof(DasKvParam) + 22 + 1]);
+  params->offset = 2048;
+  params->len = 4194512;
+  params->opcode = 0;
+  params->timeStamp = ceph_clock_now().to_nsec();
+  params->cephPoolId = 2;
+  params->algType = DAS_ALG_SEQ;
+  params->objId = 3;
+  params->imageIdLen = 22;
+  memcpy(params->imageIdBuf, obj_name.c_str(), params->imageIdLen);
+  params->handle = plugin->msg_ref;
+  params->ctx = reinterpret_cast<Objecter *>(&tObj);
+  das_req_prefetch(params);
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm_mock;
+  return;
+}
+TEST_P(ClientAdaptorTest, DasExitPushTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  Objecter tObj(g_ceph_context, NULL, NULL, NULL, 0, 0);
+  DasKvParam *params = reinterpret_cast<DasKvParam*>(new char[sizeof(DasKvParam) + 22 + 1]);
+  params->offset = 2048;
+  params->len = 4194512;
+  params->opcode = 0;
+  params->timeStamp = ceph_clock_now().to_nsec();
+  params->cephPoolId = 2;
+  params->algType = DAS_ALG_SEQ;
+  params->objId = 3;
+  params->imageIdLen = 22;
+  memcpy(params->imageIdBuf, obj_name.c_str(), params->imageIdLen);
+  params->handle = plugin->msg_ref;
+  params->ctx = reinterpret_cast<Objecter *>(&tObj);
+  das_req_prefetch(params);
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  das_req_prefetch(params);
+  return;
+}
+
+TEST_P(ClientAdaptorTest, MgrInitTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  ClientAdaptorCcmMock* ccm_mock = new ClientAdaptorCcmMock();
+  ClientAdaptorMgr* mgr = plugin->msg_ref->get_mgr();
+  std::cout << "Client Adaptor: Before DI Mgr subclass is " << mgr->name() << std::endl;
+  plugin->msg_ref->set_mgr(ccm_mock);
+  mgr = plugin->msg_ref->get_mgr();
+  int32_t res = mgr->init_mgr(nullptr);
+  bool init_flag = (res==0?true:false);
+  mgr->set_init_flag(init_flag);
+  bool is_succeed = mgr->is_init_succeed();
+  std::cout << "Mgr init result: " << is_succeed << std::endl;
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm_mock;
+  return;
+}
+
+TEST_P(ClientAdaptorTest, MgrInfoTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+
+  uint32_t num = 0;
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  uint64_t pool_id = 3;
+  uint32_t pt_index;
+
+  plugin->msg_ref->get_node_id(obj_name, pool_id, pt_index);
+  PTViewPtEntry *entry = new PTViewPtEntry();
+  NodeInfo *node_info = new NodeInfo();
+  strcpy(node_info->ipv4AddrStr, "172.0.0.1");
+  node_info->ports[0] = 68;
+  node_info->portNum = 1;
+  plugin->mgr_ref->get_pt_num(num);
+  plugin->mgr_ref->get_pt_entry(pt_index, entry);
+  delete node_info;
+  delete entry;
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+}
+
+TEST_P(ClientAdaptorTest, MgrInfoTest2)
+{
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  ClientAdaptorCcm* ccm = new ClientAdaptorCcm();
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  uint32_t num = 0;
+  uint64_t pool_id = 3;
+  uint32_t pt_index;
+  uint32_t node_id = 2;
+  plugin->msg_ref->get_node_id(obj_name, pool_id, pt_index);
+  PTViewPtEntry *entry = new PTViewPtEntry();
+  NodeInfo *node_info = new NodeInfo();
+  int32_t res = ccm->init_mgr(nullptr);
+  int32_t res1 = ccm->get_pt_num(num);
+  int32_t res2 = ccm->get_pt_entry(pt_index,entry);
+  int32_t res3 = ccm->get_node_info(node_id,node_info);
+
+  EXPECT_EQ(0, res);
+  std::cout << "Mgr init result: " << res << std::endl;
+  std::cout << "Mgr init result: " << res1 << std::endl;
+  std::cout << "Mgr init result: " << res2 << std::endl;
+  std::cout << "Mgr init result: " << res3 << std::endl;
+  delete node_info;
+  delete entry;
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm;
+  return;
+}
+
+
+INSTANTIATE_TEST_CASE_P(
+  ClientAdaptor,
+  ClientAdaptorTest,
+  ::testing::Values("client-adaptor")
+);  
diff --git a/src/test/ServerAdaptorSimulate/CMakeLists.txt b/src/test/ServerAdaptorSimulate/CMakeLists.txt
new file mode 100644
index 0000000..547e1dd
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/CMakeLists.txt
@@ -0,0 +1,9 @@
+add_executable(global_cache_server
+  global_cache_server.cc
+  global_cache_dispatcher.cc
+  )
+target_link_libraries(global_cache_server
+   global ceph-common
+   ${EXTRALIBS} 
+   ${CMAKE_DL_LIBS} 
+)
diff --git a/src/test/ServerAdaptorSimulate/global_cache_dispatcher.cc b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.cc
new file mode 100644
index 0000000..baa2d88
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.cc
@@ -0,0 +1,150 @@
+#include <string>
+#include <iostream>
+#include <iomanip>
+
+#include "include/compat.h"
+#include "global_cache_dispatcher.h"
+#include "messages/MPing.h"
+#include "messages/MDataPing.h"
+#include "messages/MOSDOpReply.h"
+#include "messages/MOSDOp.h"
+
+using namespace std;
+
+GlobalCacheDispatcher::GlobalCacheDispatcher(Messenger *msgr, int32_t rval, int32_t result):
+    Dispatcher(msgr->cct),
+    active(false),
+    messenger(msgr),
+    dcount(0),
+    rval(rval),
+    result(result)
+{
+    out_data = std::make_unique<string>(8192, 'c');
+}
+
+GlobalCacheDispatcher::~GlobalCacheDispatcher() {
+    // nothing
+}
+
+bool GlobalCacheDispatcher::ms_dispatch(Message *m)
+{
+    uint64_t dc = 0;
+
+    dc = dcount++;
+
+    ConnectionRef con = m->get_connection();
+    Messenger* msgr = con->get_messenger();
+
+    switch (m->get_type()) {
+        case CEPH_MSG_PING:
+	{
+	    cout << "Client Adaptor: " << __func__ << " msg ping " << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "connection = " << con << "peer_addr = "
+		 << con->get_peer_addr() << std::endl;
+	    break;
+	}
+	case MSG_DATA_PING:
+	{
+	    MDataPing* mdp __attribute__((unused)) = static_cast<MDataPing*>(m);
+	    cout << "Client Adaptor: " << __func__ << "msg data ping" << std::endl;
+	    ConnectionRef con = m->get_connection();
+	    con->send_message(m);
+	}
+	break;
+	case CEPH_MSG_OSD_OP:
+	{
+	    cout << "Client Adaptor: " << __func__ << " osd op msg" << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "connection = " << con << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "peer_addr = " << con->get_peer_addr() << std::endl;
+	    MOSDOp *mosd_op = static_cast<MOSDOp *>(m);
+	    mosd_op->finish_decode();
+	    cout << "Client Adaptor: " << __func__ << " MOSDOp " << *mosd_op << std::endl;
+	    cout << "Client Adaptor: " << __func__ << " pool id = " << mosd_op->get_pg().pool() << std::endl;
+	    cout << "Client Adaptor: " << __func__ << " pt id = " << mosd_op->get_pg().m_seed << std::endl;
+
+	    uint32_t index = 0;
+	    vector<OSDOp> &ops = mosd_op->ops;
+	    for (vector<OSDOp>::iterator op = ops.begin(); op != ops.end();index++, ++op) {
+	        cout << "Client Adaptor: " << __func__ << "index = " << index
+		     << " op-code = 0x" << hex << op->op.op << std::endl;
+		string cname, mname;
+		switch (op->op.op) {
+		    case CEPH_OSD_OP_READ:
+		    case CEPH_OSD_OP_SYNC_READ:
+		    case CEPH_OSD_OP_SPARSE_READ: {
+		        cout << "Client Adaptor: " << __func__ << " offset = 0x" << hex << op->op.extent.offset
+			     << "length = 0x" << hex << op->op.extent.length << std::endl;
+			if (unlikely(op->op.op == CEPH_OSD_OP_SPARSE_READ)) {
+			    std::map<uint64_t, uint64_t> extents;
+			    extents[op->op.extent.offset] = op->op.extent.length;
+			    encode(extents, op->outdata);
+			    encode(std::string_view(out_data->c_str(), op->op.extent.length), op->outdata);
+			} else {
+			    string error;
+			    cout << "Client Adaptor: " << __func__ << "read op" << std::endl;
+			    cout << "Client Adaptor: " << __func__ << "before length = 0x" << hex << op->outdata.length() << std::endl;
+			    op->outdata.read_file("/home/ceph-14.2.8/build/writefile.txt", &error);
+			    cout << "Client Adaptor: " << __func__ << " after length = 0x" << hex << op->outdata.length() << std::endl;
+			}
+			op->rval = rval;
+			cout << "Client Adaptor: " << __func__ << " op rval " << op->rval << std::endl;
+		    break;}
+		case CEPH_OSD_OP_WRITE:
+		case CEPH_OSD_OP_WRITEFULL: {
+		    cout << "Client Adaptor: " << __func__ << "offset = 0x" << hex << op->op.extent.offset
+				<< "length = 0x" << hex << op->op.extent.length << std::endl;
+		    op->indata.write_file("/home/ceph-14.2.8/build/writefile.txt");
+
+		    op->rval = rval;
+		    cout << "Client Adaptor: " << __func__ << " op rval " << op->rval << std::endl;
+		    break;}
+		case CEPH_OSD_OP_CALL:{
+		    auto bp = op->indata.cbegin();
+		    bp.copy(op->op.cls.class_len, cname);
+		    bp.copy(op->op.cls.method_len, mname);
+		    cout << "Client Adaptor: " << __func__ << " op call class: " << cname << " method: " << mname << std::endl;
+		    break;}
+		case CEPH_OSD_OP_STAT: {
+		    uint64_t psize = 0x400000;
+		    time_t ptime = 0;
+		    encode(psize, op->outdata);
+		    encode(ptime, op->outdata);
+		    break;}
+		default:
+		    break;
+		}
+	    }
+	    MOSDOpReply *reply = new MOSDOpReply(mosd_op, 0, 0, CEPH_OSD_FLAG_ACK|CEPH_OSD_FLAG_ONDISK, false);
+	    reply->claim_op_out_data(mosd_op->ops);
+	    reply->set_result(result);
+	    cout << "Client Adaptor: " << __func__ << "Return result " << reply->get_result() << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "Retry time " << mosd_op->get_retry_attempt() << std::endl;
+	    cout << "Client Adaptor: " << __func__ << " MOSDOpReply " << *reply << std::endl;
+	    mosd_op->get_connection()->send_message(reply);
+	    mosd_op->put();
+	    break;
+	}
+	default:
+	    ceph_abort();
+    }
+
+    if (unlikely(msgr->get_magic() & MSG_MAGIC_TRACE_CTR)) {
+        if (unlikely(dc % 65536) == 0) {
+	    struct timespec ts;
+	    clock_gettime(CLOCK_REALTIME_COARSE, &ts);
+	    cout << "Client Adaptor: " << __func__ << " ping " << dc << "nanos: " <<
+		    ts.tv_nsec + (ts.tv_sec * 1000000000) << std::endl;
+	}
+    }
+
+    return true;
+}
+
+bool GlobalCacheDispatcher::ms_handle_reset(Connection *con)
+{
+    return true;
+}
+
+void GlobalCacheDispatcher::ms_handle_remote_reset(Connection *con)
+{
+}    // nothing
diff --git a/src/test/ServerAdaptorSimulate/global_cache_dispatcher.h b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.h
new file mode 100644
index 0000000..b2ce439
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.h
@@ -0,0 +1,108 @@
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#ifndef GLOBAL_CACHE_DISPATCHER_H_
+#define GLOBAL_CACHE_DISPATCHER_H_
+
+#include "msg/Dispatcher.h"
+#include "msg/Messenger.h"
+
+class GlobalCacheDispatcher: public Dispatcher {
+private:
+  bool active;
+  Messenger *messenger;
+  uint64_t dcount;
+  int32_t rval;
+  int32_t result;
+  unique_ptr<string> out_data;
+
+public:
+  GlobalCacheDispatcher(Messenger *msgr, int32_t rval, int32_t result);
+  ~GlobalCacheDispatcher() override;
+  uint64_t get_dcount() { return dcount; }
+
+  void set_active() {
+    active = true;
+  };
+
+
+  bool ms_dispatch(Message *m) override;
+
+
+
+
+
+
+
+
+  void ms_handle_connect(Connection *con) override { };
+
+
+
+
+
+
+  void ms_handle_accept(Connection *con) override { };
+
+
+
+
+
+
+
+
+
+
+
+  bool ms_handle_reset(Connection *con) override;
+
+
+
+
+
+
+
+
+
+
+  void ms_handle_remote_reset(Connection *con) override;
+
+  bool ms_handle_refused(Connection *con) override { return false; }
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+  
+  bool ms_get_authorizer(int dest_type, AuthAuthorizer **a) override {
+    return false;
+  };
+
+  int ms_handle_authentication(Connection *con) override {
+    return 1;
+  }
+};
+
+#endif
+
diff --git a/src/test/ServerAdaptorSimulate/global_cache_server.cc b/src/test/ServerAdaptorSimulate/global_cache_server.cc
new file mode 100644
index 0000000..486825a
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/global_cache_server.cc
@@ -0,0 +1,131 @@
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#include <sys/types.h>
+
+#include <iostream>
+#include <string>
+
+using namespace std;
+
+#include "common/config.h"
+#include "msg/Messenger.h"
+#include "common/Timer.h"
+#include "common/ceph_argparse.h"
+#include "global/global_init.h"
+#include "global/signal_handler.h"
+#include "perfglue/heap_profiler.h"
+#include "common/address_helper.h"
+#include "global_cache_dispatcher.h"
+#include "auth/DummyAuth.h"
+#include "msg/async/AsyncMessenger.h"
+
+#define dout_subsys ceph_subsys_global_cache
+
+void usage(ostream& out)
+{
+  out << "usage: global_cache_server [options]\n"
+    "options:\n"
+    "--rval -ErrorCode\n"
+        "--result -ErrorCode\n"
+	;
+}
+
+int main(int argc, const char **argv)
+{
+    vector<const char*> args;
+    Messenger *messenger;
+    Dispatcher *dispatcher;
+    vector<const char*>::iterator arg_iter;
+    string val;
+    int32_t rval = 0;
+    int32_t result = 0;
+    entity_addr_t bind_addr; 
+
+    string addr = "localhost";
+    string port = "1234";
+
+    cout << "Client Adaptor: " << __func__ << " Global Cache Server starting..." << std::endl;
+
+    argv_to_vec(argc, argv, args);
+
+    auto cct = global_init(NULL, args, CEPH_ENTITY_TYPE_ANY,
+		    CODE_ENVIRONMENT_DAEMON,
+		    CINIT_FLAG_NO_DEFAULT_CONFIG_FILE);
+
+    for (arg_iter = args.begin(); arg_iter != args.end();) {
+      if (ceph_argparse_witharg(args, arg_iter, &val, "--rval", (char*) NULL)){
+        rval = atoi(val.c_str());
+      } else if (ceph_argparse_witharg(args, arg_iter, &val, "--result", (char*) NULL)){
+        result = atoi(val.c_str());
+      } else {
+      ++arg_iter;
+      }
+    };
+
+    if (!args.empty()) {
+      cerr << "What is this? -- " << args[0] << std::endl;
+      usage(cerr);
+      exit(1);
+    }
+
+    string dest_str = "tcp://";
+    dest_str += addr;
+    dest_str += ":";
+    dest_str += port;
+    entity_addr_from_url(&bind_addr, dest_str.c_str());
+  entity_addrvec_t bind_addrs(bind_addr);
+
+  string ms_type = g_conf().get_val<std::string>("ms_type");
+  cout << "Client Adaptor: " << __func__ << " ms_type: " << ms_type << std::endl;
+    messenger = Messenger::create(g_ceph_context, ms_type,
+		    entity_name_t::OSD(-1),
+		    "global_cache_server",
+		    0 /* nonce */,
+		    0 /* flags */);
+
+    DummyAuthClientServer dummy_auth(g_ceph_context);
+  dummy_auth.auth_registry.refresh_config();
+  messenger->set_auth_client(&dummy_auth);
+    messenger->set_auth_server(&dummy_auth);
+    messenger->set_magic(MSG_MAGIC_TRACE_CTR);
+    messenger->set_default_policy(Messenger::Policy::stateless_server(0));
+
+
+  bind_addr.set_type(entity_addr_t::TYPE_MSGR2);
+    int32_t r = messenger->bind(bind_addr);
+    if(r < 0)
+       goto out;
+
+
+
+    common_init_finish(g_ceph_context);
+
+    dispatcher = new GlobalCacheDispatcher(messenger, rval, result);
+    dispatcher->ms_set_require_authorizer(false);
+
+    messenger->add_dispatcher_head(dispatcher);
+    messenger->start();
+
+  cout << "Client Adaptor: " << __func__ << " server conn = "
+	    << static_cast<AsyncMessenger *>(messenger)->lookup_conn(bind_addrs) << std::endl;
+    messenger->wait();
+
+
+    delete messenger;
+
+out:
+    cout << "Client Adaptor: " << __func__ << " Simple Server exit" << std::endl;
+    return r;
+}
diff --git a/src/test/librbd/io/test_mock_ImageRequestWQ.cc b/src/test/librbd/io/test_mock_ImageRequestWQ.cc
index 50daa83..209740a 100644
--- a/src/test/librbd/io/test_mock_ImageRequestWQ.cc
+++ b/src/test/librbd/io/test_mock_ImageRequestWQ.cc
@@ -58,6 +58,11 @@ struct ImageDispatchSpec<librbd::MockTestImageCtx> {
     return s_instance;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  AioCompletion* get_completion() {
+    return aio_comp;
+  }
+#endif
   MOCK_CONST_METHOD0(is_write_op, bool());
   MOCK_CONST_METHOD0(start_op, void());
   MOCK_CONST_METHOD0(send, void());
diff --git a/src/test/messenger/simple_client.cc b/src/test/messenger/simple_client.cc
index ba7ed2b..8b1eee7 100644
--- a/src/test/messenger/simple_client.cc
+++ b/src/test/messenger/simple_client.cc
@@ -30,6 +30,7 @@ using namespace std;
 #include "common/address_helper.h"
 #include "message_helper.h"
 #include "simple_dispatcher.h"
+#include "auth/DummyAuth.h"
 
 #define dout_subsys ceph_subsys_simple_client
 
@@ -59,7 +60,7 @@ int main(int argc, const char **argv)
 	std::string addr = "localhost";
 	std::string port = "1234";
 
-	int n_msgs = 50;
+	int n_msgs = 1;
 	int n_dsize = 0;
 
 	struct timespec ts;
@@ -101,12 +102,20 @@ int main(int argc, const char **argv)
 	  "dest port " << port << " " <<
 	  "initial msgs (pipe depth) " << n_msgs << " " <<
 	  "data buffer size " << n_dsize << std::endl;
-
-	messenger = Messenger::create(g_ceph_context, g_conf().get_val<std::string>("ms_type"),
-				      entity_name_t::MON(-1),
+             g_ceph_context->_conf.set_val("auth_cluster_required", "none");
+             g_ceph_context->_conf.set_val("auth_service_required", "none");
+             g_ceph_context->_conf.set_val("auth_client_required", "none");
+	     string ms_type = g_conf().get_val<std::string>("ms_type");
+	     cout << "Client Adaptor: ms_type: " << ms_type << std::endl;
+	messenger = Messenger::create(g_ceph_context, ms_type,
+ 				      entity_name_t::CLIENT(-1),
 				      "client",
 				      getpid(), 0);
 
+	DummyAuthClientServer dummy_auth(g_ceph_context);
+        	dummy_auth.auth_registry.refresh_config();
+	messenger->set_auth_client(&dummy_auth);
+		messenger->set_auth_server(&dummy_auth);
 	// enable timing prints
 	messenger->set_magic(MSG_MAGIC_TRACE_CTR);
 	messenger->set_default_policy(Messenger::Policy::lossy_client(0));
@@ -115,10 +124,16 @@ int main(int argc, const char **argv)
 	dest_str += addr;
 	dest_str += ":";
 	dest_str += port;
+		cout << "Client Adaptor: address = " << dest_str << std::endl;
 	entity_addr_from_url(&dest_addr, dest_str.c_str());
+	//dest_addr.set_type(entity_addr_t::TYPE_LEGACY);
+	dest_addr.set_type(entity_addr_t::TYPE_MSGR2);
+		cout << "Client Adaptor: legacy address = " << dest_addr.get_legacy_str() << std::endl;
 	entity_addrvec_t dest_addrs(dest_addr);
+		cout << "Client Adaptor: vec legacy address = " << dest_addrs.get_legacy_str() << std::endl;
 
 	dispatcher = new SimpleDispatcher(messenger);
+	dispatcher->ms_set_require_authorizer(false);
 	messenger->add_dispatcher_head(dispatcher);
 
 	dispatcher->set_active(); // this side is the pinger
@@ -127,7 +142,12 @@ int main(int argc, const char **argv)
 	if (r < 0)
 		goto out;
 
-	conn = messenger->connect_to_mon(dest_addrs);
+	conn = messenger->connect_to_osd(dest_addrs);
+		std::cout << "Client Adaptor: conn = " << conn << std::endl;
+
+		while (!conn->is_connected()) {
+			nanosleep(&ts, NULL);
+		}
 
 	// do stuff
 	time_t t1, t2;
@@ -144,6 +164,12 @@ int main(int argc, const char **argv)
 	    m = new_simple_ping_with_data("simple_client", n_dsize);
 	  }
 	  conn->send_message(m);
+		std::cout << "Client Adaptor: client message conn = " << m->get_connection() << std::endl;
+		std::cout << "Client Adaptor: peer_addr = " << conn->get_peer_addr() << std::endl;
+		entity_addrvec_t peer_addr_vec = *(m->get_connection()->peer_addrs);
+		for (auto it : peer_addr_vec.v) {
+			std::cout << "Client Adaptor: peer_addr = " << it.get_legacy_str() << std::endl;
+		}	
 	}
 
 	// do stuff
diff --git a/src/test/messenger/simple_dispatcher.cc b/src/test/messenger/simple_dispatcher.cc
index b13958d..7e1011b 100644
--- a/src/test/messenger/simple_dispatcher.cc
+++ b/src/test/messenger/simple_dispatcher.cc
@@ -17,6 +17,8 @@
 #include "simple_dispatcher.h"
 #include "messages/MPing.h"
 #include "messages/MDataPing.h"
+#include "messages/MOSDOpReply.h"
+#include "messages/MOSDOp.h"
 
 SimpleDispatcher::SimpleDispatcher(Messenger *msgr) :
   Dispatcher(msgr->cct),
@@ -42,16 +44,37 @@ bool SimpleDispatcher::ms_dispatch(Message *m)
 
   switch (m->get_type()) {
   case CEPH_MSG_PING:
+  {
+    std::cout << "Client Adaptor: msg ping " << std::endl;  
+    std::cout << "Client Adaptor: conn =  " << con << "peer_addr = " << con->get_peer_addr() <<  std::endl;  
+
+
+
+
     break;
+  }
   case MSG_DATA_PING:
   {
     MDataPing* mdp __attribute__((unused)) = static_cast<MDataPing*>(m);
+    std::cout << "Client Adaptor: msg data ping " << std::endl;  
     //cout << "MDataPing " << mdp->tag << " " << mdp->counter << std::endl;
     //mdp->get_data().hexdump(cout);
     ConnectionRef con = m->get_connection();
     con->send_message(m);
   }
     break;
+  case CEPH_MSG_OSD_OP:
+  {
+    std::cout << "Client Adaptor: osd op msg " << std::endl;  
+    std::cout << "Client Adaptor: conn = " << con << std::endl;  
+    std::cout << "Client Adaptor: peer_addr = " << con->get_peer_addr() << std::endl;  
+    MOSDOp *osd_op = static_cast<MOSDOp*>(m);
+    MOSDOpReply *reply = new MOSDOpReply(osd_op, 0, 0, 0, false);
+    std::cout << "Client Adaptor: connection " << m->get_connection() << std::endl;  
+     m->get_connection()->send_message(reply);
+     m->put();
+     break;
+   }
   default:
     ceph_abort();
   }
@@ -66,7 +89,7 @@ bool SimpleDispatcher::ms_dispatch(Message *m)
   } /* trace ctr */
 
 
-  con->send_message(m);
+  //con->send_message(m);
 
   //m->put();
 
diff --git a/src/test/messenger/simple_server.cc b/src/test/messenger/simple_server.cc
index 8b85f3a..34feff1 100644
--- a/src/test/messenger/simple_server.cc
+++ b/src/test/messenger/simple_server.cc
@@ -28,6 +28,8 @@ using namespace std;
 #include "perfglue/heap_profiler.h"
 #include "common/address_helper.h"
 #include "simple_dispatcher.h"
+#include "auth/DummyAuth.h"
+#include "msg/async/AsyncMessenger.h"
 
 #define dout_subsys ceph_subsys_simple_server
 
@@ -72,17 +74,26 @@ int main(int argc, const char **argv)
 	dest_str += ":";
 	dest_str += port;
 	entity_addr_from_url(&bind_addr, dest_str.c_str());
+		entity_addrvec_t bind_addrs(bind_addr);
 
-	messenger = Messenger::create(g_ceph_context, g_conf().get_val<std::string>("ms_type"),
-				      entity_name_t::MON(-1),
+		string ms_type = g_conf().get_val<std::string>("ms_type");
+		cout << "Client Adaptor: ms_type: " << ms_type << std::endl;
+	messenger = Messenger::create(g_ceph_context, ms_type,
+				      entity_name_t::OSD(-1),
 				      "simple_server",
 				      0 /* nonce */,
 				      0 /* flags */);
 	// enable timing prints
+	DummyAuthClientServer dummy_auth(g_ceph_context);
+        	dummy_auth.auth_registry.refresh_config();
+		messenger->set_auth_client(&dummy_auth);
+	messenger->set_auth_server(&dummy_auth);
 	messenger->set_magic(MSG_MAGIC_TRACE_CTR);
 	messenger->set_default_policy(
 	  Messenger::Policy::stateless_server(0));
 
+
+		bind_addr.set_type(entity_addr_t::TYPE_MSGR2);
 	r = messenger->bind(bind_addr);
 	if (r < 0)
 		goto out;
@@ -92,9 +103,12 @@ int main(int argc, const char **argv)
 	common_init_finish(g_ceph_context);
 
 	dispatcher = new SimpleDispatcher(messenger);
+	dispatcher->ms_set_require_authorizer(false);
 
 	messenger->add_dispatcher_head(dispatcher); // should reach ready()
 	messenger->start();
+
+	std::cout << "Client Adaptor: server conn = " << static_cast<AsyncMessenger *>(messenger)->lookup_conn(bind_addrs) << std::endl;
 	messenger->wait(); // can't be called until ready()
 
 	// done
diff --git a/src/vstart.sh b/src/vstart.sh
index 37aa28b..a146de3 100644
--- a/src/vstart.sh
+++ b/src/vstart.sh
@@ -752,6 +752,8 @@ EOF
             local uuid=`uuidgen`
             echo "add osd$osd $uuid"
 	    OSD_SECRET=$($CEPH_BIN/ceph-authtool --gen-print-key)
+
+	    OSD_SECRET=`echo ${OSD_SECRET} | awk '{print $NF}'`
 	    echo "{\"cephx_secret\": \"$OSD_SECRET\"}" > $CEPH_DEV_DIR/osd$osd/new.json
             ceph_adm osd new $uuid -i $CEPH_DEV_DIR/osd$osd/new.json
 	    rm $CEPH_DEV_DIR/osd$osd/new.json
